                                                                  Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                           delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1                             delta_net_4layer_adam_state_update_in_for_loop  0.337757        0.089304      0.024024  0.384297      0.023897        0.196143
2              delta_net_4layer_add_local_softmax_self_attention_in_for_loop  0.431751        0.998026      0.205195  0.727279      0.994433        0.952803
3   delta_net_4layer_cosine_softmax_attention_error_gated_update_in_for_loop  0.444827        0.996742      0.165556  0.649914      0.988256        0.959016
4                      delta_net_4layer_cosine_softmax_attention_in_for_loop  0.436265        0.996512      0.162116  0.666553      0.990794        0.934717
5                            delta_net_4layer_decay_state_update_in_for_loop  0.435427        0.998480      0.218365  0.363693      0.996212        0.932825
6                      delta_net_4layer_error_gated_state_update_in_for_loop  0.439931        0.998649      0.227001  0.414150      0.998151        0.898605
7                      delta_net_4layer_gelu_on_prediction_error_in_for_loop  0.433138        0.998555      0.204038  0.398156      0.998466        0.905582
8                  delta_net_4layer_hybrid_softmax_raw_attention_in_for_loop  0.386774        0.999059      0.529823  0.612872      0.998455        0.908444
9                        delta_net_4layer_layernorm_state_update_in_for_loop  0.425510        0.995487      0.166240  0.357337      0.992839        0.873239
10                        delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
11                               delta_net_4layer_normalize_keys_in_for_loop  0.414407        0.997862      0.159351  0.339369      0.998346        0.951903
12                         delta_net_4layer_rmsprop_state_update_in_for_loop  0.333525        0.084800      0.030725  0.378087      0.031335        0.123251
13                                      delta_net_4layer_rope_qk_in_for_loop  0.418859        0.997551      0.176376  0.330314      0.995076        0.972541
14                      delta_net_4layer_sigmoid_similarity_gate_in_for_loop  0.429010        0.998919      0.180641  0.387742      0.998661        0.922861
15                            delta_net_4layer_softmax_attention_in_for_loop  0.427559        0.987641      0.160105  0.779396      0.977399        0.939016
16                            delta_net_4layer_topk_error_update_in_for_loop  0.334018        0.992124      0.166969  0.404012      0.990607        0.693629
17                                                    gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
1. Current Experimental Landscape
• DeltaNet baseline (delta_net_4layer) already outperforms Gated-DeltaNet on Compress (+0.075) but lags on Fuzzy Recall (-0.094), Memorize (-0.183) and Selective Copy (-0.047).  
• Best Fuzzy-Recall so far is 0.530 (hybrid_softmax_raw_attention) but that variant hurts Compress (-0.055) and Memorize (-0.015 vs Gated).  
• Optimisation-style state-updates (Adam/RMSprop) and heavy gating consistently collapse performance – indicating that too much disturbance of the delta update harms the stable memories needed for MAD.  
• Light-weight attention tweaks (softmax, cosine, RoPE) improve some metrics without destroying others, suggesting that the bottleneck is HOW the chunk attends to past tokens, not the delta rule itself.  

2. Observed Failure Modes
• Dense dot-product attention inside the loop produces a broad weight distribution that mixes relevant and irrelevant states—hurting Fuzzy and Memorize (noise/interference sensitive).  
• Some variants improve Fuzzy Recall but at the cost of Compress, implying over-fitting to noisy-key tasks while reducing capacity for long sequences.  

3. Insight & New Direction: Sparse Probabilistic Attention
A principled way to reduce interference is to retain only the most salient memory traces per query.  Softmax provides a well-behaved probability simplex; Top-K pruning turns it into a sparse, high-contrast pattern selector.  This should:  
• Increase signal-to-noise ratio → better Fuzzy & Noisy Recall.  
• Reduce destructive averaging across many keys → assist Memorize/Compress.  
• Keep computation cheap ( K≪chunk_size ).  

4. Implemented Modification (within the for-loop)
Step-by-step in naive.py:
 1) Compute causal dot-product scores; mask future positions.
 2) Apply temperature-scaled softmax to obtain probabilities.
 3) Keep only the Top-K (=4) probabilities per query; zero-out the rest (scatter).  Gradients flow through the surviving entries, so the model can still learn which K to favour.
 4) Use these sparse probs to mix the u_i vectors (DeltaNet’s error-filtered values).
 5) Perform the usual causal Delta state update AFTER output.

No other parts of the original algorithm were touched; complexity is O(K) additional.

5. Expected Impact vs MAD Tasks
• Fuzzy/Noisy Recall: Higher robustness because attention now ignores low-similarity (noisy) keys.
• Memorize: Less interference between unrelated key-value pairs, enabling more accurate out-of-order retrieval.
• Compress: Retains strong long-range memories by preventing small but cumulative overwrites from low-relevance positions.
• Selective Copy & Context Recall: Should remain high; sparse probabilities do not harm when the correct entry is among Top-K (very likely because of softmax sharpening).

6. Code Locations Written
• New variant saved to /mnt/iem-nas/home/liuyixiu/AI_Archer/mad-lab/naive/delta_net_4layer_topk_softmax_attention_in_for_loop.py
• Core implementation replacing naive.py under flash-linear-attention so subsequent training/evaluation will use it.

7. Next Steps for Verification
• Re-train/evaluate on MAD; tune K (2-8) and chunk_size synergy.
• If further interference observed, couple Top-K with a learnable temperature or adaptive K.
