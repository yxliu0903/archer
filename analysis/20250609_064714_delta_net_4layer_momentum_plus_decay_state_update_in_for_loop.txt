                                                                  Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                           delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1                             delta_net_4layer_adam_state_update_in_for_loop  0.337757        0.089304      0.024024  0.384297      0.023897        0.196143
2              delta_net_4layer_adaptive_gated_softmax_attention_in_for_loop  0.424852        0.998082      0.167910  0.748651      0.993177        0.962474
3              delta_net_4layer_add_local_softmax_self_attention_in_for_loop  0.431751        0.998026      0.205195  0.727279      0.994433        0.952803
4   delta_net_4layer_cosine_softmax_attention_error_gated_update_in_for_loop  0.444827        0.996742      0.165556  0.649914      0.988256        0.959016
5                      delta_net_4layer_cosine_softmax_attention_in_for_loop  0.436265        0.996512      0.162116  0.666553      0.990794        0.934717
6                            delta_net_4layer_decay_state_update_in_for_loop  0.435427        0.998480      0.218365  0.363693      0.996212        0.932825
7                      delta_net_4layer_error_gated_state_update_in_for_loop  0.439931        0.998649      0.227001  0.414150      0.998151        0.898605
8                 delta_net_4layer_frobenius_renorm_state_update_in_for_loop  0.439120        0.998412      0.208326  0.389314      0.998346        0.944087
9                      delta_net_4layer_gelu_on_prediction_error_in_for_loop  0.433138        0.998555      0.204038  0.398156      0.998466        0.905582
10                 delta_net_4layer_hybrid_softmax_raw_attention_in_for_loop  0.386774        0.999059      0.529823  0.612872      0.998455        0.908444
11                delta_net_4layer_layernorm_qk_before_attention_in_for_loop  0.417799        0.995964      0.168702  0.338787      0.995637        0.689787
12                       delta_net_4layer_layernorm_state_update_in_for_loop  0.425510        0.995487      0.166240  0.357337      0.992839        0.873239
13                        delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
14                               delta_net_4layer_normalize_keys_in_for_loop  0.414407        0.997862      0.159351  0.339369      0.998346        0.951903
15                         delta_net_4layer_rmsprop_state_update_in_for_loop  0.333525        0.084800      0.030725  0.378087      0.031335        0.123251
16                                      delta_net_4layer_rope_qk_in_for_loop  0.418859        0.997551      0.176376  0.330314      0.995076        0.972541
17       delta_net_4layer_sigmoid_error_forget_gate_state_update_in_for_loop  0.425961        0.997450      0.153117  0.401083      0.995883        0.877183
18                      delta_net_4layer_sigmoid_similarity_gate_in_for_loop  0.429010        0.998919      0.180641  0.387742      0.998661        0.922861
19                            delta_net_4layer_softmax_attention_in_for_loop  0.427559        0.987641      0.160105  0.779396      0.977399        0.939016
20                            delta_net_4layer_topk_error_update_in_for_loop  0.334018        0.992124      0.166969  0.404012      0.990607        0.693629
21                       delta_net_4layer_topk_softmax_attention_in_for_loop  0.394498        0.973047      0.172306  0.436038      0.970196        0.966246
22                                                    gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908

1. Quick view of latest experiment table (MAD-scores are fractions of 100):
• Baseline delta_net_4layer ≈ 0.44 Compress / 1.00 Ctx-Recall / 0.22 Fuzzy / 0.40 Memorize / 1.00 Noisy / 0.95 Sel-Copy.
• Best-so-far variants:
  – hybrid_softmax_raw_attention: large Fuzzy boost (0.53) and Memorize 0.61 (> Gated-DeltaNet 0.59) but Sel-Copy still far below 1.0.
  – cosine_softmax_* family: stable Compress (~0.44) but Fuzzy still ≤0.21, Memorize ≤0.67.
  – rope_qk_in_for_loop: very strong Sel-Copy (0.97) and Compress (0.42) but weak Memorize (0.33) & Fuzzy (0.18).
• Gradient-based optimisers on the state (Adam/RMSprop) collapse performance across all tasks – confirming that heavy parameter-style optimisation inside the forward pass is too unstable.
• Lightweight gating / decay ideas (decay_state_update, error_gated_state_update, sigmoid_error_forget_gate) keep most scores near baseline but don’t move the weakest metric (Fuzzy Recall) much.
• Conclusion: 
  a) Key bottleneck is Fuzzy Recall (noise-robust retrieval) and secondarily Memorize.  
  b) Approaches that make the update too aggressive (Adam, RMSprop) introduce instability; overly soft updates (simple decay) cannot strengthen new associations fast enough.

2. Insights about previous modifications
• Softmax / cosine attention variants help Fuzzy recall because they sharpen similarity matching, but this sometimes harms Memorize by introducing competition between closely-spaced keys.
• Additional layernorms stabilise gradients but don’t solve noise robustness.
• Simple exponential decay prevents memory explosion but also weakens long-range retention, hence Memorize drops.
• Momentum-only (delta_net_4layer_momentum_state_update) smooths noise but without any forgetting the state keeps accumulating error, hurting long sequences (Ctx-Recall is lower).
• Therefore a *joint* mechanism that (i) filters noise (momentum) and (ii) forgets gradually (decay) is still missing.

3. NEW improvement implemented (inside the for-loop)
Modification name:  delta_net_4layer_momentum_plus_decay_state_update_in_for_loop
Algorithmic changes (in naïve.py and a separate file for history):
• Keep a velocity tensor m (same shape as state S).  Update it as an exponential moving average of the error projection:
      m = μ * m + (1 - μ) * e_i   with μ=0.9
  This smooths the high-frequency component of the error, boosting robustness on Fuzzy / Noisy recall.
• Apply controlled forgetting before integrating the momentum term:
      S = (1 - γ) * S + m         with γ=0.01
  A small decay γ removes stale traces, mitigating interference and helping Compress/Selective-Copy.
• Rest of the pipeline (attention, causal masking, output computation) remains unchanged, preserving DeltaNet’s strengths.
The change respects causal order because we write to S only after computing this step’s output.

Expected benefits
• Momentum removes spurious fluctuations due to noisy keys ⇒ higher Fuzzy & Noisy Recall.
• Mild decay avoids runaway growth and excessive interference ⇒ better Memorize & Selective-Copy.
• Combination should surpass Gated-DeltaNet’s average by improving tasks where original DeltaNet lagged, while keeping its existing perfect (≈1.0) scores on context/noisy recall.

4. Files written
• /mad-lab/naive/delta_net_4layer_momentum_plus_decay_state_update_in_for_loop.py  (new variant for tracking)
• /flash-linear-attention/fla/ops/delta_rule/naive.py (overwritten with new logic)

Please retrain/evaluate this variant – empirical verification will show whether the theorised gains materialise.