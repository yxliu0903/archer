                                                                  Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                           delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1                             delta_net_4layer_adam_state_update_in_for_loop  0.337757        0.089304      0.024024  0.384297      0.023897        0.196143
2   delta_net_4layer_cosine_softmax_attention_error_gated_update_in_for_loop  0.444827        0.996742      0.165556  0.649914      0.988256        0.959016
3                      delta_net_4layer_cosine_softmax_attention_in_for_loop  0.436265        0.996512      0.162116  0.666553      0.990794        0.934717
4                            delta_net_4layer_decay_state_update_in_for_loop  0.435427        0.998480      0.218365  0.363693      0.996212        0.932825
5                      delta_net_4layer_error_gated_state_update_in_for_loop  0.439931        0.998649      0.227001  0.414150      0.998151        0.898605
6                      delta_net_4layer_gelu_on_prediction_error_in_for_loop  0.433138        0.998555      0.204038  0.398156      0.998466        0.905582
7                  delta_net_4layer_hybrid_softmax_raw_attention_in_for_loop  0.386774        0.999059      0.529823  0.612872      0.998455        0.908444
8                        delta_net_4layer_layernorm_state_update_in_for_loop  0.425510        0.995487      0.166240  0.357337      0.992839        0.873239
9                         delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
10                         delta_net_4layer_rmsprop_state_update_in_for_loop  0.333525        0.084800      0.030725  0.378087      0.031335        0.123251
11                                      delta_net_4layer_rope_qk_in_for_loop  0.418859        0.997551      0.176376  0.330314      0.995076        0.972541
12                      delta_net_4layer_sigmoid_similarity_gate_in_for_loop  0.429010        0.998919      0.180641  0.387742      0.998661        0.922861
13                            delta_net_4layer_softmax_attention_in_for_loop  0.427559        0.987641      0.160105  0.779396      0.977399        0.939016
14                            delta_net_4layer_topk_error_update_in_for_loop  0.334018        0.992124      0.166969  0.404012      0.990607        0.693629
15                                                    gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
1. Current Landscape on the MAD Benchmark
• Baseline comparison
  – DeltaNet-4L already beats Mamba & GLA on most memory/noise tasks but under-performs Gated-DeltaNet on:
     • Fuzzy Recall (21.7 % vs 31.0 %)
     • Memorize     (40.3 % vs 58.7 %)
     • Selective Copy(95.0 % vs 99.7 %)
  – Gated-DeltaNet sacrifices Compress accuracy (36.7 % vs 44.2 %).

• Effect of past modifications
  ┌───────────────────────────────────────────────────────────────────────────┐
  │ Key gains                                                                │
  ├──────────────────┬──────────────────────────┬────────────────────────────┤
  │ Modification      │ Biggest uplift           │ Biggest regression         │
  ├──────────────────┼──────────────────────────┼────────────────────────────┤
  │ softmax_attention │ Memorize ↑ to 77.9 %     │ Fuzzy Recall still low (16 %) │
  │ hybrid_softmax+raw│ Fuzzy Recall ↑ to 52.9 % │ Compress drops to 38.7 %   │
  │ cosine_softmax    │ Memorize ↑ (66-65 %)     │ Fuzzy Recall remains ~16 % │
  │ error-gated       │ Fuzzy ↑ 22.7 %           │ Little help on Memorize    │
  │ decay / momentum  │ Slight stability gains   │ Noisy/overall unchanged    │
  └──────────────────┴──────────────────────────┴────────────────────────────┘
  Insights:
  • Local soft-max attention helps precise key matching → Memorize ↑ but hurts Compress & stability.
  • Error- or decay-based state tweaks give marginal robustness but don’t close the Memorize gap.
  • No single variant so far surpasses Gated-DeltaNet on ALL six tasks; Compress & Fuzzy remain the bottlenecks.

2. Bottleneck Diagnosis
  • Delta rule’s outer-product memory is excellent for long-range/global associations.
  • However, it stores only one global state S; recent fine-grained details inside the same chunk compete for that capacity.
  • Tasks such as Compress & Fuzzy Recall require BOTH: (i) exact short-range pattern matching and (ii) long-range error-driven memory.

3. New Proposal (implemented)
  Name  : delta_net_4layer_add_local_softmax_self_attention_in_for_loop
  Idea  : Hybridise DeltaNet with a SHORT-RANGE causal soft-max path computed inside the existing for-loop.
  Mechanics (per chunk i)
     1. Keep the original Delta computations (o_inter + dot_raw @ u_i).
     2. In parallel, compute causal scaled-dot soft-max attention **within the current chunk only**:
             attn_local = softmax((q_i k_i^T)/√d_k)  (masked future positions)
             o_local   = attn_local @ v_raw
     3. Fuse outputs:  o_i = o_inter + dot_raw @ u_i + o_local
     4. Update S with the usual Delta rule (after output ⇒ causal correctness).

  Why it should help
   • o_local supplies high-resolution, low-noise context → better Compress & Fuzzy Recall.
   • S still handles global associative memory → Memorize & Noisy Recall stay strong.
   • Computation remains chunk-local, O(L) memory, satisfies flash-linear design.

4. Expected Impact w.r.t. Gated-DeltaNet
  • Compress: local soft-max directly reconstructs recent tokens → should climb above 0.44 baseline while not regressing like earlier wide soft-max variants (scope limited to chunk).
  • Fuzzy Recall: fuzzy key perturbations often map to nearby vectors; soft-max promotes nearest-neighbour lookup → accuracy ↑ beyond 0.31.
  • Memorize & Noisy Recall: preserved or improved via unchanged Delta state.
  • Selective Copy: local attention helps pick targets out of clutter.
  If these gains materialise, the model can surpass Gated-DeltaNet on all six tasks.

5. Implementation Notes
  ✓ Modification confined to two small blocks inside the for-loop.
  ✓ No change to function name or call signature.
  ✓ State update S executed after output (causality preserved).
  ✓ Uses masked_fill(−inf) + softmax for numerical stability.
  ✓ Adds < 3 matrix multiplies per chunk – negligible overhead.

6. File Updates
  • /mnt/iem-nas/home/liuyixiu/AI_Archer/mad-lab/naive/delta_net_4layer_add_local_softmax_self_attention_in_for_loop.py (new file)
  • /mnt/iem-nas/home/liuyixiu/mad-lab/flash-linear-attention/fla/ops/delta_rule/naive.py (overwritten)

Run new experiments with this variant; if predictions hold, it should become the first DeltaNet version to beat Gated-DeltaNet across the board.
