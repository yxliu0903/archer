                                                                  Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                           delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1                             delta_net_4layer_adam_state_update_in_for_loop  0.337757        0.089304      0.024024  0.384297      0.023897        0.196143
2   delta_net_4layer_cosine_softmax_attention_error_gated_update_in_for_loop  0.444827        0.996742      0.165556  0.649914      0.988256        0.959016
3                      delta_net_4layer_cosine_softmax_attention_in_for_loop  0.436265        0.996512      0.162116  0.666553      0.990794        0.934717
4                            delta_net_4layer_decay_state_update_in_for_loop  0.435427        0.998480      0.218365  0.363693      0.996212        0.932825
5                      delta_net_4layer_error_gated_state_update_in_for_loop  0.439931        0.998649      0.227001  0.414150      0.998151        0.898605
6                  delta_net_4layer_hybrid_softmax_raw_attention_in_for_loop  0.386774        0.999059      0.529823  0.612872      0.998455        0.908444
7                        delta_net_4layer_layernorm_state_update_in_for_loop  0.425510        0.995487      0.166240  0.357337      0.992839        0.873239
8                         delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
9                          delta_net_4layer_rmsprop_state_update_in_for_loop  0.333525        0.084800      0.030725  0.378087      0.031335        0.123251
10                                      delta_net_4layer_rope_qk_in_for_loop  0.418859        0.997551      0.176376  0.330314      0.995076        0.972541
11                      delta_net_4layer_sigmoid_similarity_gate_in_for_loop  0.429010        0.998919      0.180641  0.387742      0.998661        0.922861
12                            delta_net_4layer_softmax_attention_in_for_loop  0.427559        0.987641      0.160105  0.779396      0.977399        0.939016
13                                                    gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
Key Findings from accuracies_df.csv
1. Baseline DeltaNet (delta_net_4layer)
   • Excels at Context-Recall (≈1.0), Noisy-Recall (≈0.997) and Selective-Copy (≈0.95)
   • Weak at Fuzzy-Recall (≈0.22) and Memorize (≈0.40)
   • Compress is mid-level (≈0.44)

2. Previous Modification Clusters
   • Softmax / Cosine-Softmax Attention variants raised Memorize up to ≈0.78 and ≈0.67 but hardly helped Fuzzy-Recall.
   • Error-gated & Decay variants kept robustness (Context, Noisy) but gave only marginal Fuzzy-Recall gains.
   • Hybrid Softmax-Raw Attention produced the best Fuzzy-Recall so far (≈0.53) while keeping other scores competitive, proving that manipulating the error signal/attention can indeed boost noise-sensitive tasks.
   • Optimisation-based (Adam/RMSprop) hurt every metric → large, noisy gradient estimates destabilise the delta rule.
   • RoPE only slightly improved Selective-Copy; LayerNorm or Sigmoid-gates showed negligible effect.

3. Overall Gap vs. Gated-DeltaNet
   • Gated-DeltaNet still wins on Compress (+0.37→0.366 vs 0.44), Fuzzy-Recall (0.31 vs ≥0.22–0.53) and Memorize (0.59 vs ≤0.78 but trade-offs elsewhere).  Its gating evidently reduces over-writing and keeps long-term structure.

Insight Summary
• Boosting Fuzzy-/Noisy-Recall requires more sophisticated handling of small, noisy errors—not merely attention tweaks.
• Memorize benefits from higher-order, non-linear transformations of the error but too much optimisation noise ruins stability.
• Successful variants preserved core delta rule causality and did not disturb the outer-product update.

NEW Improvement (Implemented)
Name  : delta_net_4layer_gelu_on_prediction_error_in_for_loop
Idea  : Apply a GELU non-linearity to the prediction error u_i **inside the for-loop** before it is fed to both the output path and the memory update.
Mathematical Motivation
• u_i = v_pred_err currently passes *all* deviations linearly.  GELU acts like a soft threshold: it suppresses small (likely noisy) errors but keeps large, structured errors almost unchanged.  This:
  – Filters high-frequency noise → better Fuzzy & Noisy Recall
  – Prevents over-writing correct associations → better Memorize & Compress
  – Retains differentiability and smooth gradients unlike hard thresholding.
Implementation Location
• Exactly after computing u_i and before o_inter/attn update – satisfies “inside for-loop, after output is stored, causal”.
Expected Task Impact
• Higher noise-robustness for Fuzzy/Noisy Recall due to error attenuation.
• More stable long-range storage (Compress) because small jitters don’t pollute S.
• Potential Memorize boost because strong errors still reinforce correct pairs.

Why Different from Earlier Attempts
• Previous attempts added softmax, cosine similarity, momentum, decay, etc. None non-linearly reshaped the *error itself*.  This is a new axis (signal-domain non-linearity) rather than attention/state-management tweaks.

Code Locations Written
• /mnt/iem-nas/home/liuyixiu/AI_Archer/mad-lab/naive/delta_net_4layer_gelu_on_prediction_error_in_for_loop.py
• /mnt/iem-nas/home/liuyixiu/mad-lab/flash-linear-attention/fla/ops/delta_rule/naive.py

Both files compile and keep the original function signature (delta_rule_chunkwise) unchanged.
