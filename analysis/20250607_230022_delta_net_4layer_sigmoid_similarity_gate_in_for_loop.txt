                                              Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                       delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1         delta_net_4layer_adam_state_update_in_for_loop  0.337757        0.089304      0.024024  0.384297      0.023897        0.196143
2        delta_net_4layer_decay_state_update_in_for_loop  0.435427        0.998480      0.218365  0.363693      0.996212        0.932825
3  delta_net_4layer_error_gated_state_update_in_for_loop  0.439931        0.998649      0.227001  0.414150      0.998151        0.898605
4    delta_net_4layer_layernorm_state_update_in_for_loop  0.425510        0.995487      0.166240  0.357337      0.992839        0.873239
5     delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
6      delta_net_4layer_rmsprop_state_update_in_for_loop  0.333525        0.084800      0.030725  0.378087      0.031335        0.123251
7         delta_net_4layer_softmax_attention_in_for_loop  0.427559        0.987641      0.160105  0.779396      0.977399        0.939016
8                                 gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
Experiment Results Overview (accuracies_df.csv)
──────────────────────────────────────────────
                           Compress  CtxtRecall  FuzzyRecall  Memorize  NoisyRecall  SelCopy  Avg
-------------------------------------------------------------------------------------------------
DeltaNet-4L (baseline)         0.442       0.999        0.217     0.403        0.997    0.950  0.668
Decay-state update             0.435       0.998        0.218     0.364        0.996    0.933  0.657
Error-gated update             0.440       0.999        0.227     0.414        0.998    0.899  0.663
LayerNorm in loop              0.426       0.995        0.166     0.357        0.993    0.873  0.635
Momentum update                0.432       0.992        0.144     0.370        0.994    0.879  0.635
Softmax-attn (best so far)     0.428       0.988        0.160     0.779        0.977    0.939  0.712
Adam / RMSProp variants        ↘ drastic performance collapse (context & noise robustness lost)
Gated-DeltaNet-4L (target)     0.367       1.000        0.310     0.587        1.000    0.997  0.710

Key Insights
1. DeltaNet family excels at Context Recall, Noisy Recall, and Selective Copy (≥0.95) but still struggles on
   • Fuzzy Recall (≈0.22)
   • Memorize (≈0.40)
2. Pure optimisation-style updates (Adam, RMSProp) destroy its main strength (long-range memory), proving that
   gradient-noise is harmful inside the tight for-loop.
3. Adding a Softmax inside the loop lifted Memorize strongly (0.78) – showing that *better weighting of intra-chunk
   information* helps associative storage – but Fuzzy Recall stayed very low (0.16) because the model becomes more
   aggressive and less noise-aware.
4. Gated-DeltaNet’s success stems from *adaptive control* over state updates. It increases Fuzzy Recall (0.31) and
   Memorize (0.59) while retaining noise robustness.

New Modification – Similarity-Gated Delta Rule
──────────────────────────────────────────────
Motivation
• We need a mechanism that boosts Memorize yet also protects against key noise → introduce *per-token gates* that
  scale with query–key similarity.
• High similarity → trust new info (helps precise memorisation and compress). Low similarity → fall back on existing
  memory (improves Fuzzy/Noisy Recall).

Implementation (inside the for-loop only)
1. Compute similarity sim = (q_i • k_i) and gate = σ(sim).
2. Output o_i = (1−g)·(q_i S)  +  g·(Attn u_i).
3. State update S ← S + g·k_i^T u_i (gated delta update).

Expected Effect
• Memorize: when the right key comes again, similarity is high ⇒ strong update and strong output ⇒ accuracy ↑.
• Fuzzy & Noisy Recall: noisy keys have lower similarity ⇒ gate ≈0 ⇒ model relies on pre-stored associations instead
  of writing noise, so robustness ↑.
• Other tasks: Context Recall, Selective Copy and Noisy Recall should remain almost intact because gating defaults to
  previous behaviour when similarity is high.

Hence the variant should close the gap in Fuzzy Recall while at least keeping the gains in Memorize, with negligible
cost.

All requirements satisfied
✓ Modification placed **inside the for-loop**.
✓ State update happens **after** output computation.
✓ Function name unchanged; only one file touched.
✓ Provides clear mathematical motivation & targets model weaknesses.
✓ File saved to both required paths.
