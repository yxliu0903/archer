                                           Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                    delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1  delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
2                              gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
1. Summary of Current Results (accuracies_df.csv)
   • delta_net_4layer: strong on Compress (44%), Context-Recall (99%) and Noisy-Recall (99%) but weak on Fuzzy-Recall (22%) and Memorize (40%). Avg ≈ 66.8.
   • delta_net_4layer_momentum_state_update_in_for_loop: momentum lowered Fuzzy-Recall (14%) and Memorize (37%); overall avg ≈ 63.5 → momentum harmed performance (likely over-smoothed updates, losing precision).
   • gated_delta_net_4layer: best average ≈ 71.0, thanks to higher Memorize (59%) and Fuzzy-Recall (31%), but sacrifices Compress (37%).

   Insights w.r.t MAD tasks
   • Compress & Noisy-Recall mainly test long-range retention and denoising. All three variants already score ≥99% on Noisy-Recall; DeltaNet also leads Compress.
   • Fuzzy-Recall & Memorize require storing many similar key–value pairs without interference. DeltaNet’s pure additive outer-product accumulates errors here, while Gated DeltaNet curbs interference via decay gating – hence its edge on these two tasks.
   • Momentum update further blurred the memory trace (good for stability, bad for discrimination), worsening precisely the interference-sensitive tasks.

2. Rationale for NEW Modification
   Goal: keep DeltaNet’s strengths (Compress, Context, Noisy) while closing the gap on Fuzzy-Recall & Memorize.
   Key limitation: unlimited growth of state S → memory saturation.
   Proposed remedy: inexpensive exponential decay inside the for-loop
        S ← γ · S  +  kᵀ · Δv       with  γ∈(0,1)
   • γ<1 softly forgets stale associations, freeing capacity and reducing overlap between similar keys.
   • Unlike previous momentum (which blended *updates*), decay directly shrinks *stored state* before adding the new outer-product, preserving sharpness of recent bindings.
   • Mathematical motivation: keeps Frobenius norm of S bounded (||S_t||≤γ^t||S_0||+Σγ^{t-i}||ΔS_i||) – mitigating blow-up and interference.

3. Implemented Change (in both code locations)
   File: fla/ops/delta_rule/naive.py  (+ identical copy under naive/)
   • Inside the main chunk loop, added one line:
         S = decay_rate * S + k_i.transpose(-1,-2) @ u_i    # γ=0.9 by default
   • decay_rate is a function argument (defaults to 0.9; setting 1.0 reproduces original behaviour).
   • Rest of algorithm unchanged, ensuring causality and computational cost O(H·D²) per chunk as before.

4. Expected Effects
   • Fuzzy-Recall & Memorize: reduced cross-talk should lift accuracy, potentially surpassing Gated DeltaNet while retaining DeltaNet’s high scores elsewhere.
   • Compress: slight drop possible but γ close to 1 keeps long-range traces; empirical tuning of γ offers trade-off.
   • Implementation is lightweight – no extra parameters (unless we later learn γ).

5. Next Steps
   • Grid-search γ∈{0.8,0.9,0.95} on validation set.
   • Optionally make γ learnable or key-dependent (gating by relevance) for further gains.

Overall, the exponential-decay variant directly targets DeltaNet’s principal weakness—excessive retention/interference—without compromising its error-driven learning mechanism, and is fundamentally different from the previous momentum attempt.
