                                                                  Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                           delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1                             delta_net_4layer_adam_state_update_in_for_loop  0.337757        0.089304      0.024024  0.384297      0.023897        0.196143
2              delta_net_4layer_adaptive_gated_softmax_attention_in_for_loop  0.424852        0.998082      0.167910  0.748651      0.993177        0.962474
3              delta_net_4layer_add_local_softmax_self_attention_in_for_loop  0.431751        0.998026      0.205195  0.727279      0.994433        0.952803
4   delta_net_4layer_cosine_softmax_attention_error_gated_update_in_for_loop  0.444827        0.996742      0.165556  0.649914      0.988256        0.959016
5                      delta_net_4layer_cosine_softmax_attention_in_for_loop  0.436265        0.996512      0.162116  0.666553      0.990794        0.934717
6                            delta_net_4layer_decay_state_update_in_for_loop  0.435427        0.998480      0.218365  0.363693      0.996212        0.932825
7                      delta_net_4layer_error_gated_state_update_in_for_loop  0.439931        0.998649      0.227001  0.414150      0.998151        0.898605
8                      delta_net_4layer_gelu_on_prediction_error_in_for_loop  0.433138        0.998555      0.204038  0.398156      0.998466        0.905582
9                  delta_net_4layer_hybrid_softmax_raw_attention_in_for_loop  0.386774        0.999059      0.529823  0.612872      0.998455        0.908444
10                delta_net_4layer_layernorm_qk_before_attention_in_for_loop  0.417799        0.995964      0.168702  0.338787      0.995637        0.689787
11                       delta_net_4layer_layernorm_state_update_in_for_loop  0.425510        0.995487      0.166240  0.357337      0.992839        0.873239
12                        delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
13                               delta_net_4layer_normalize_keys_in_for_loop  0.414407        0.997862      0.159351  0.339369      0.998346        0.951903
14                         delta_net_4layer_rmsprop_state_update_in_for_loop  0.333525        0.084800      0.030725  0.378087      0.031335        0.123251
15                                      delta_net_4layer_rope_qk_in_for_loop  0.418859        0.997551      0.176376  0.330314      0.995076        0.972541
16                      delta_net_4layer_sigmoid_similarity_gate_in_for_loop  0.429010        0.998919      0.180641  0.387742      0.998661        0.922861
17                            delta_net_4layer_softmax_attention_in_for_loop  0.427559        0.987641      0.160105  0.779396      0.977399        0.939016
18                            delta_net_4layer_topk_error_update_in_for_loop  0.334018        0.992124      0.166969  0.404012      0.990607        0.693629
19                       delta_net_4layer_topk_softmax_attention_in_for_loop  0.394498        0.973047      0.172306  0.436038      0.970196        0.966246
20                                                    gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
1. Overview of Previous Experiments
•   Baselines
    – DeltaNet-4-layer (original): Avg ≈ 0.668 – strong on Context (0.999) & Noisy Recall (0.997) but weak on Fuzzy (0.217) and Memorize (0.403).
    – Gated-DeltaNet-4-layer: Avg ≈ 0.710 – gains mainly on Fuzzy (0.310) & Memorize (0.587) while retaining very high Context/Noisy/Selective scores.

•   Best Variants So Far
    ① hybrid_softmax_raw_attention_in_for_loop …… Avg ≈ 0.739 (▲ 0.029 vs Gated-DeltaNet) – Fuzzy Recall leaps to 0.530, Memorize 0.613; Compress drops slightly.
    ② adaptive_gated_softmax_attention_in_for_loop …… Avg ≈ 0.716 – major Memorize boost (0.749) but Fuzzy still low (0.168).

•   General Trends
    – Adding any form of softmax–style local attention generally improves Memorize and sometimes Fuzzy but can hurt Compress.
    – Error-gated or decay-style state updates help Compress but seldom raise Fuzzy/Memorize enough to beat Gated-DeltaNet.
    – Optimization-based updates (Adam/RMSProp) dramatically destabilised learning → very low Context & Noisy scores.

2. Effectiveness of Tried Modifications
Capability-wise:
    • Compress      ↗ with error-gated / decay variants.
    • Fuzzy Recall  ↗ only when we change similarity measure (cosine) or mix raw & softmax attention. Best so far 0.530.
    • Memorize      ↗ mainly by variants that insert softmax attention (hybrid or adaptive-gated) – up to 0.749.
    • Context / Noisy Recall are already near-perfect; most variants preserved them except Adam/RMSProp.
Bottleneck: still no single variant that simultaneously keeps very high Compress AND ≥ 0.5 Fuzzy AND ≥ 0.65 Memorize.

3. New Idea Implemented Today: Error-Adaptive-Temperature Softmax (inside the for-loop)
Motivation
    • If current memory predicts well (small error) we should attend sharply (low temperature) to reinforce the right key–value binding.
    • If error is large we should spread attention (high temperature) to explore alternative candidates – useful for Fuzzy & Noisy Recall.
Algorithmic Change (only inside the for-loop)
    τ  = 1 + λ‖u_i‖        (λ = err_scale, default 0.5)
    A  = softmax((q_i k_iᵀ) / τ)
    where u_i = current prediction error after inverse projection.
    – Everything else (delta-rule state update, causality ordering) untouched.
Expected Benefits
    • Dynamic sharpening/flattening balances memorisation stability (Compress, Memorize) with flexibility (Fuzzy, Noisy).
    • Uses the already-computed error term; negligible extra compute.
    • Fundamentally different from previous static-temperature softmax or gating approaches tried so far.

4. File Changes
    a)   Added new implementation to
         /mnt/iem-nas/home/liuyixiu/AI_Archer/mad-lab/naive/delta_net_4layer_error_adaptive_temperature_softmax_attention_in_for_loop.py
    b)   Over-wrote main operator file
         /mnt/iem-nas/home/liuyixiu/mad-lab/flash-linear-attention/fla/ops/delta_rule/naive.py

5. Next-Step Hypothesis
If the hypothesis holds we should observe:
    • ≥ 0.44 Compress (on par with the best non-gated)
    • ≥ 0.55 Fuzzy (beat 0.530 record)
    • ≥ 0.70 Memorize (close to adaptive_gated_softmax)
    • Context / Noisy / Selective remain ≈ 1.0
That would give an average > 0.74 and surpass both Gated-DeltaNet and previous best variant.
