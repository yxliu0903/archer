                                                                  Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                           delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1                             delta_net_4layer_adam_state_update_in_for_loop  0.337757        0.089304      0.024024  0.384297      0.023897        0.196143
2   delta_net_4layer_cosine_softmax_attention_error_gated_update_in_for_loop  0.444827        0.996742      0.165556  0.649914      0.988256        0.959016
3                      delta_net_4layer_cosine_softmax_attention_in_for_loop  0.436265        0.996512      0.162116  0.666553      0.990794        0.934717
4                            delta_net_4layer_decay_state_update_in_for_loop  0.435427        0.998480      0.218365  0.363693      0.996212        0.932825
5                      delta_net_4layer_error_gated_state_update_in_for_loop  0.439931        0.998649      0.227001  0.414150      0.998151        0.898605
6                  delta_net_4layer_hybrid_softmax_raw_attention_in_for_loop  0.386774        0.999059      0.529823  0.612872      0.998455        0.908444
7                        delta_net_4layer_layernorm_state_update_in_for_loop  0.425510        0.995487      0.166240  0.357337      0.992839        0.873239
8                         delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
9                          delta_net_4layer_rmsprop_state_update_in_for_loop  0.333525        0.084800      0.030725  0.378087      0.031335        0.123251
10                                      delta_net_4layer_rope_qk_in_for_loop  0.418859        0.997551      0.176376  0.330314      0.995076        0.972541
11                      delta_net_4layer_sigmoid_similarity_gate_in_for_loop  0.429010        0.998919      0.180641  0.387742      0.998661        0.922861
12                            delta_net_4layer_softmax_attention_in_for_loop  0.427559        0.987641      0.160105  0.779396      0.977399        0.939016
13                                                    gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
1. Printing & Observing the Current Results (scores are in [0,1])
--------------------------------------------------------------------------------
Model                                             Cmp   Ctx   Fuz   Mem   Noi   Sel
--------------------------------------------------------------------------------
delta_net_4layer                                  0.44  1.00  0.22  0.40  1.00  0.95
Δ-Net + decay                                     0.44  1.00  0.22  0.36  1.00  0.93
Δ-Net + error-gate                                0.44  1.00  0.23  0.41  1.00  0.90
Δ-Net + layer-norm                                0.43  1.00  0.17  0.36  0.99  0.87
Δ-Net + momentum                                  0.43  0.99  0.14  0.37  0.99  0.88
Δ-Net + RoPE-qk                                   0.42  1.00  0.18  0.33  1.00  0.97
Δ-Net + softmax-attn                              0.43  0.99  0.16  0.78  0.98  0.94  ◀ best Mem
Δ-Net + hybrid raw+softmax attn                   0.39  1.00  0.53  0.61  1.00  0.91  ◀ best Fuz
Δ-Net + cosSoft + gate                            0.44  1.00  0.17  0.65  0.99  0.96
Δ-Net + Adam-style                                0.34  0.09  0.02  0.38  0.02  0.20  ⇩ broke
Δ-Net + RMSProp                                   0.33  0.08  0.03  0.38  0.03  0.12  ⇩ broke
Gated-Delta-Net-4L (reference)                    0.37  1.00  0.31  0.59  1.00  1.00
--------------------------------------------------------------------------------
Key insights
• Original DeltaNet already wins Context & Noise tasks (>0.99) but lags in Fuzzy-Recall and Memorize.
• Softmax–based local attention variants noticeably raise Memorize (0.78) and Fuzzy-Recall (0.53) without hurting strong tasks.
• Pure optimisation-style state updates (Adam/RMSProp) destabilise learning – large drops everywhere.
• RoPE improves Selective-Copy but not enough for Fuzzy-Recall.

Effectiveness of previous modifications
• Adding a softmax path is the only family that consistently improved the weak tasks.
• Gating, decay and momentum offered marginal gains (<0.02).
• Therefore the next change should amplify the benefit of softmax attention while preserving the error-driven core.

2. NEW modification we introduce (implemented inside the for-loop)
   delta_net_4layer_local_softmax_value_attention_in_for_loop
   ──────────────────────────────────────────────────────────
   Idea: add a parallel **local, causal softmax attention over the current chunk** that uses the *original* (un-β-gated) values.  The chunk-sized Transformer micro-module copies tokens verbatim, complementing the error-driven Delta path.
   Mathematical form (per chunk i):
       A_raw    =  q_i k_i^T                       (causal mask, no softmax)
       A_local  =  softmax(mask_fill(q_i k_i^T))   (causal softmax)
       u_i      =  u_pre[i]  −  w_pre[i] S         (same as before)
       o_i      =  q_i S   +   A_raw u_i   +   A_local v_i   ←  NEW term
       S        =  S + k_i^T u_i                   (state update unchanged)
   Expected benefits
   • Precise token copying → boosts Selective-Copy & Memorize.
   • Softmax pathway provides sharper association → higher Fuzzy-Recall.
   • Keeps DeltaNet’s superior noise / context performance intact.

3. Implementation summary
   • Only one extra ~c×c softmax per chunk: negligible cost.
   • No new learnable parameters; safe drop-in replacement.
   • Respects causality: state S updated *after* output.

The modified code was written to
   • /mnt/iem-nas/home/liuyixiu/AI_Archer/mad-lab/naive/delta_net_4layer_local_softmax_value_attention_in_for_loop.py
   • /mnt/iem-nas/home/liuyixiu/mad-lab/flash-linear-attention/fla/ops/delta_rule/naive.py
so the training / evaluation pipeline will automatically use it.

4. What to look for in the next experiment
   • Δ-Net + local softmax should raise Fuzzy-Recall (>0.55) and Memorize (>0.8) while keeping Context & Noisy ≈1, Compress ≥0.40, Selective ≥0.96.
   • That would yield an overall average >0.75, surpassing Gated-Delta-Net in every task.
