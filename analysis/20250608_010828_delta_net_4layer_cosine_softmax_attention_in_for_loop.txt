                                              Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                       delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1         delta_net_4layer_adam_state_update_in_for_loop  0.337757        0.089304      0.024024  0.384297      0.023897        0.196143
2        delta_net_4layer_decay_state_update_in_for_loop  0.435427        0.998480      0.218365  0.363693      0.996212        0.932825
3  delta_net_4layer_error_gated_state_update_in_for_loop  0.439931        0.998649      0.227001  0.414150      0.998151        0.898605
4    delta_net_4layer_layernorm_state_update_in_for_loop  0.425510        0.995487      0.166240  0.357337      0.992839        0.873239
5     delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
6      delta_net_4layer_rmsprop_state_update_in_for_loop  0.333525        0.084800      0.030725  0.378087      0.031335        0.123251
7   delta_net_4layer_sigmoid_similarity_gate_in_for_loop  0.429010        0.998919      0.180641  0.387742      0.998661        0.922861
8         delta_net_4layer_softmax_attention_in_for_loop  0.427559        0.987641      0.160105  0.779396      0.977399        0.939016
9                                 gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
Experiment Results Overview (accuracies 0-1)
------------------------------------------------
                                 Compress  Ctxt-Recall  Fuzzy  Memorize  Noisy  SelCopy   Avg
1  delta_net_4layer               0.442      0.999     0.217   0.403    0.997   0.950   0.668
2  + Adam state-update            0.338      0.089     0.024   0.384    0.024   0.196   0.176
3  + Exponential decay            0.435      0.998     0.218   0.364    0.996   0.933   0.657
4  + Error-gated update           0.440      0.999     0.227   0.414    0.998   0.899   0.663
5  + LayerNorm                    0.426      0.995     0.166   0.357    0.993   0.873   0.635
6  + Momentum                     0.432      0.992     0.144   0.370    0.994   0.879   0.635
7  + RMSProp                      0.334      0.085     0.031   0.378    0.031   0.123   0.163
8  + Sigmoid-sim gate             0.429      0.999     0.181   0.388    0.999   0.923   0.653
9  + Softmax-attn                 0.428      0.988     0.160   0.779    0.977   0.939   0.712 ★
10 gated_delta_net_4layer         0.367      0.999     0.310   0.587    0.999   0.997   0.710

Key Insights
1. Baseline DeltaNet already excels at Context & Noisy Recall (> 0.99) but struggles with Fuzzy Recall (0.22) and Memorize (0.40).
2. Optimiser-style updates (Adam, RMSProp, Momentum) hurt almost every task – they introduce high-variance updates that destabilise the error-driven memory.
3. Simple architectural tweaks (decay, error-gate, LayerNorm, sigmoid gating) kept overall performance similar but did not solve Fuzzy/Memorize weaknesses.
4. Injecting a softmax over intra-chunk attention (+ Softmax-attn) dramatically increased Memorize (→ 0.78) while keeping other scores high, pushing the average (0.712) marginally above Gated DeltaNet (0.710).  The probabilistic weighting evidently helps the model pick correct key-value lines inside each chunk.
5. However, Fuzzy Recall remains low (~0.16–0.23) – dot-product similarity is still sensitive to the 10 % noise perturbation.

New Proposal – Cosine-Similarity + Softmax Attention (implemented here)
----------------------------------------------------------------------
Motivation: Replace magnitude-dependent dot-product by scale-invariant cosine similarity before softmax.  This should:
• Improve robustness to key noise ⇒ directly targets the Fuzzy & Noisy Recall gap.
• Preserve the softmax benefit for Memorize.
Implementation specifics
• Change lives strictly inside the for-loop: compute `q_norm`, `k_norm`, `sim = q_norm @ k_normᵀ`; apply causal mask; softmax.
• All other computations (error term `u_i`, outer-product state update `S`) stay untouched, maintaining causality and computational cost.
Predicted Effects
• Higher Fuzzy & Noisy Recall thanks to scale-invariance.
• Retain Memorize gains from softmax weighting.
• Little impact on Context Recall/Compress which are already near-perfect.

Suggested Future Directions (beyond one-file change)
• Combine cosine-softmax with an adaptive β-gate that decreases for already-well-remembered pairs (confidence-based forgetting).
• Lightweight RoPE on keys only to assist length extrapolation without extra parameters.

Overall, the new variant tackles DeltaNet’s remaining weak spot (noise robustness) while keeping its strengths, setting the stage to decisively surpass Gated DeltaNet across all MAD tasks.