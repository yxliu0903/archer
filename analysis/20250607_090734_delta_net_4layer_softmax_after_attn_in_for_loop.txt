                                           Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                    delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1  delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
2                              gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
Experiment Results Summary (accuracies_df.csv)
------------------------------------------------
                Compress  C.Recall  Fuzzy  Memorize  Noisy  Sel.Copy  Average
DeltaNet-4L        0.442     0.999  0.217     0.403  0.997      0.950   0.668
ΔNet + Momentum    0.432     0.992  0.144     0.370  0.994      0.879   0.635
Gated-ΔNet-4L      0.367     0.999  0.310     0.587  0.999      0.997   0.710

Key Insights
1. DeltaNet already excels at tasks that favour error-driven noise filtering (Noisy-Recall 99.7%, Context-Recall 99.9%, Selective-Copy 95%).
2. Its major weaknesses are Fuzzy-Recall (21.7%) and Memorize (40.3%).  These require
   • tolerant matching under mild key perturbations (Fuzzy)  
   • stable scalar scale during iterative state updates to avoid interference (Memorize).
3. Adding Momentum (previous trial) slightly helped Compress but hurt the two weak tasks further, dropping the global average to 0.635 (-5 pts vs. baseline).  The accumulated velocity evidently introduced additional variance that harmed fuzzy matching.
4. Gated-DeltaNet improves Memorize (+18 pts) and Fuzzy-Recall (+9 pts) through decay-gating but at the cost of Compress and overall complexity.

Why the New Modification (Softmax-Normalised Intra-Chunk Attention) Targets the Gaps
-----------------------------------------------------------------------------------
• Scale Stability – softmax turns raw dot-product scores into a probability simplex, ensuring the weighted prediction-error vector \(attn@u_i\) has bounded magnitude ⇒ mitigates exploding variance that previously corrupted memory slots, directly benefiting Memorize & Compress.
• Noise Tolerance – relative, not absolute, similarity is emphasised; even when a key is perturbed by 10 % (Fuzzy-Recall) or Gaussian noise (Noisy-Recall) it can still receive high probability if it remains the *closest* match.
• Minimal overhead – no new learnable parameters, GPU-friendly; modification is confined to the loop so the model remains causal and identical everywhere else.
• Complementary to Delta Rule – preserves error-driven update mechanism but normalises the *selection* part; addresses exactly the two weakest metrics while keeping the strong ones intact.

Expectations vs. Gated-DeltaNet
• Should close the Fuzzy-Recall gap without the heavier exponential-decay gate.  
• Keeps Compress higher than Gated variant because softmax does not actively forget.
• Maintains DeltaNet’s strengths on Context & Noisy recall.
If realised, the average should surpass 0.71.

Implementation Notes
• Location: inside the for-loop, right after computing q_i·k_i^T and before mixing u_i.  Masked causal positions receive −1e30 so their softmax weight ≈0, preserving causality.
• State update S is untouched and still applied *after* output generation.
• Two files were written:
  – mad-lab/naive/delta_net_4layer_softmax_after_attn_in_for_loop.py  
  – flash-linear-attention/fla/ops/delta_rule/naive.py (overwriting old naïve version)
Both contain identical functional logic, documented with rationales.

Next Steps
• Re-train or fine-tune this variant on the MAD benchmark.  
• Monitor Fuzzy-Recall & Memorize for expected uplift; ensure Compress does not regress.
• If further gains are required, combine softmax with a *mild* decay gate (α≈0.1) or layer-norm on S.
