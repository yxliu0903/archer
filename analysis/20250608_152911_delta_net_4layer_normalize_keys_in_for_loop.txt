                                                                  Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                           delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1                             delta_net_4layer_adam_state_update_in_for_loop  0.337757        0.089304      0.024024  0.384297      0.023897        0.196143
2              delta_net_4layer_add_local_softmax_self_attention_in_for_loop  0.431751        0.998026      0.205195  0.727279      0.994433        0.952803
3   delta_net_4layer_cosine_softmax_attention_error_gated_update_in_for_loop  0.444827        0.996742      0.165556  0.649914      0.988256        0.959016
4                      delta_net_4layer_cosine_softmax_attention_in_for_loop  0.436265        0.996512      0.162116  0.666553      0.990794        0.934717
5                            delta_net_4layer_decay_state_update_in_for_loop  0.435427        0.998480      0.218365  0.363693      0.996212        0.932825
6                      delta_net_4layer_error_gated_state_update_in_for_loop  0.439931        0.998649      0.227001  0.414150      0.998151        0.898605
7                      delta_net_4layer_gelu_on_prediction_error_in_for_loop  0.433138        0.998555      0.204038  0.398156      0.998466        0.905582
8                  delta_net_4layer_hybrid_softmax_raw_attention_in_for_loop  0.386774        0.999059      0.529823  0.612872      0.998455        0.908444
9                        delta_net_4layer_layernorm_state_update_in_for_loop  0.425510        0.995487      0.166240  0.357337      0.992839        0.873239
10                        delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
11                         delta_net_4layer_rmsprop_state_update_in_for_loop  0.333525        0.084800      0.030725  0.378087      0.031335        0.123251
12                                      delta_net_4layer_rope_qk_in_for_loop  0.418859        0.997551      0.176376  0.330314      0.995076        0.972541
13                      delta_net_4layer_sigmoid_similarity_gate_in_for_loop  0.429010        0.998919      0.180641  0.387742      0.998661        0.922861
14                            delta_net_4layer_softmax_attention_in_for_loop  0.427559        0.987641      0.160105  0.779396      0.977399        0.939016
15                            delta_net_4layer_topk_error_update_in_for_loop  0.334018        0.992124      0.166969  0.404012      0.990607        0.693629
16                                                    gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
1. Quick recap of the raw numbers (mean-normalised to 0–1)
----------------------------------------------------------------
                                     COMP   CONTXT   FUZZY   MEM  NOISE  SEL
DeltaNet-base (4L)                    0.442  0.999   0.217  0.403 0.997 0.950
Gated-DeltaNet (4L)                   0.367  0.999   0.310  0.587 0.999 0.997
– best prior DeltaNet variants –
ℹ add_local_softmax_self_attention    0.432  0.998   0.205  0.727 0.994 0.953
ℹ cosine_softmax_attention_error_gate 0.445  0.997   0.166  0.650 0.988 0.959
ℹ hybrid_softmax_raw_attention        0.387  0.999   0.530  0.613 0.998 0.908

Key observations
• DeltaNet already dominates “Context”, “Noisy” (≈1.0) – we keep those.
• The two bottlenecks that make Gated-DeltaNet better are Fuzzy-Recall and Memorise.
  – Local/hybrid soft-max variants clearly help Fuzzy but sometimes hurt Compress.
  – Adam/RMSProp-style optimisation in the loop is catastrophic (destroys noise-robustness).
• ‘Hybrid softmax raw’ lifts Fuzzy to 0.53 (vs 0.31 for Gated) and beats Gated by +0.03 on mean accuracy (≈0.74 vs 0.71) – but still trails on Selective-Copy (0.91 vs 0.997).

Why the previous tweaks plateaued
• Almost all experiments kept the *magnitude* of keys untouched.  When we add softmax-like normalisation on the scoring side, any scale differences in keys creep back in during the outer-product update, causing interference and hurting Compress / Selective-Copy.
• Optimiser-style momentum/Adam magnifies this scale problem → divergence.

New idea – within-loop L2 key normalisation
------------------------------------------
Motivation
• Make similarity purely angular (cosine) → aligns with how the MAD tasks are generated (random unit vectors + noise). This is especially important for Fuzzy- and Noisy-Recall where we want orientation, not length, to drive retrieval.
• By using the *same* normalised keys in the outer-product update we prevent very large keys from overwriting memory disproportionally (a root cause of Compress degradation we saw before).
• Computationally cheap, differentiable, and does not break causal ordering.
Implementation highlights
• Added one line inside the for-loop: `k_i_norm = F.normalize(k_i, p=2, dim=-1, eps=1e-6)`
• Replace `k_i` with `k_i_norm` in (a) local attention, (b) state update.
• Everything else (pre-computed factors, padding logic, causal update after output) untouched ⇒ drop-in replacement.
Expected gains
• Fuzzy-Recall: angular similarity → higher tolerance to the 10 % perturbation.
• Memorise: reduced catastrophic interference because each new key has unit energy.
• Compress & Selective-Copy: should at least match baseline because normalisation does not degrade temporal information, yet keeps S stable.
• Almost cost-free (one normalise call per chunk).

2. Additional improvement avenues (future work)
• Combine key normalisation with the best performing ‘hybrid softmax raw’ attention (could be toggled by a flag).
• Light-weight gating that blends raw vs. softmax-weighted error based on norm of prediction error.
• Position-wise learned temperature τ to trade off Compress vs. Fuzzy adaptively.

3. Verdict
The new variant directly attacks the two weak spots (Fuzzy, Memorise) while leaving the strong points of DeltaNet intact.  Given the previous empirical trends we expect it to surpass Gated-DeltaNet’s mean score and hopefully close the gap on Selective-Copy.
