                                              Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                       delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1        delta_net_4layer_decay_state_update_in_for_loop  0.435427        0.998480      0.218365  0.363693      0.996212        0.932825
2  delta_net_4layer_error_gated_state_update_in_for_loop  0.439931        0.998649      0.227001  0.414150      0.998151        0.898605
3     delta_net_4layer_momentum_state_update_in_for_loop  0.432281        0.992050      0.144458  0.369833      0.993707        0.879067
4         delta_net_4layer_softmax_attention_in_for_loop  0.427559        0.987641      0.160105  0.779396      0.977399        0.939016
5                                 gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908
Experiment Result Synopsis (accuracies ∈[0,1])
--------------------------------------------------------------------
                  Compress  Ctx-Recall  Fuzzy   Memorize  Noisy   Sel.Copy   Avg
Baseline Δ-Net     0.442     0.999      0.217   0.403     0.997   0.950      0.668
+ Decay-S          0.435     0.998      0.218   0.364     0.996   0.933      0.658
+ Error-Gate       0.440     0.999      0.227   0.414     0.998   0.899      0.663
+ Momentum         0.432     0.992      0.144   0.370     0.994   0.879      0.635
+ Softmax-Attn     0.428     0.988      0.160   0.779     0.977   0.939      0.712
Gated Δ-Net        0.367     0.999      0.310   0.587     0.999   0.997      0.710

Key Insights
1. Softmax-Attention is the only variant that beats Gated DeltaNet on average (+0.002).  The gain is driven primarily by a huge Memorize boost (+0.38 absolute).  Compress & Fuzzy/Noisy Recall, however, slip a bit, hinting that the logits–softmax step improves discrimination inside a chunk but slightly weakens long-range accumulation.
2. Decay and Error-Gate keep performance close to the baseline; they neither sufficiently strengthen weak spots (Compress, Memorize) nor hurt robustness.  They confirm that simply adding decay or another gate is not enough without adaptive magnitude control.
3. Momentum hurts Fuzzy Recall and Compress, showing that uniform momentum amplifies earlier errors instead of correcting them.
4. Overall picture:  DeltaNet already owns Ctx- and Noisy-Recall (>0.99) but trails on Memorize & Fuzzy Recall.  The most promising direction is therefore to stabilise updates (help with noisy/fuzzy) while letting the model accumulate repetitive patterns (help Memorize) without erasing older information (help Compress).

Proposed NEW Modification (not tried before)
Adam-style adaptive optimisation of the external memory state S, executed INSIDE the existing for-loop.
Mathematical idea
• Treat the outer-product g_t = k_t^T @ u_t as a ‘gradient’.  Maintain first (m) and second (v) moments and apply bias-corrected Adam update:
    m ← β₁ m + (1−β₁) g_t
    v ← β₂ v + (1−β₂) g_t²
    S ← S + lr · m̂ / (√v̂ + ε)   (after producing the output, to keep causality)
Benefits expected
• Per-cell adaptive learning rates dampen high-variance cells (noise) and accelerate low-variance cells (repeated facts) ⇒ should boost both Fuzzy/Noisy-Recall and Memorize without degrading Compress.
• Adds only two auxiliary tensors (m, v) of the same size as S; computational overhead is marginal.

Implementation highlights
• Only 12 lines inside the “for i in …” loop were changed; the public API and function name remain intact.
• Bias correction uses a cheap scalar power per step.
• Hyper-parameters (β₁, β₂, ε, lr) are exposed but default to standard Adam values.

File updates
✓ /mad-lab/naive/delta_net_4layer_adam_state_update_in_for_loop.py  (new variant)
✓ /flash-linear-attention/fla/ops/delta_rule/naive.py  (main library version replaced)

The variant name accurately captures what/where was modified.

Expected outcome
• Stronger Fuzzy & Noisy Recall via variance normalisation.
• Enhanced Memorize & Compress from adaptive learning-rate accumulation.
• Should comfortably surpass the current Gated DeltaNet average while preserving DeltaNet’s core strengths.
