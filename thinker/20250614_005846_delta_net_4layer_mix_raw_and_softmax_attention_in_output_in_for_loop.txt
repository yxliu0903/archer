                                                                  Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                           delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1  delta_net_4layer_add_error_based_sigmoid_gate_to_state_update_in_for_loop  0.438554        0.996988      0.151126  0.378910      0.993787        0.856096
2    delta_net_4layer_add_exponential_decay_to_S_before_update_in_chunk_loop  0.430250        0.998632      0.174114  0.400849      0.997338        0.950205
3                      delta_net_4layer_add_layer_norm_to_qk_before_chunking  0.427726        0.998013      0.133315  0.270263      0.996733        0.979295
4                  delta_net_4layer_add_momentum_to_state_update_in_for_loop  0.432103        0.994521      0.164317  0.386064      0.992176        0.880826
5                            delta_net_4layer_add_rope_to_qk_before_chunking  0.450334        0.997173      0.160970  0.325375      0.995668        0.975949
6          delta_net_4layer_add_softmax_to_intra_chunk_attention_in_for_loop  0.436064        0.985717      0.160863  0.779965      0.971766        0.951675
7                                                     gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908

20250614_005846

--- Insights ---
Insights
========
Key observations from latest accuracies_df.csv
--------------------------------------------
1. DeltaNet variants that applied **pure softmax normalisation** (Attempt-2) produced the single biggest jump on Memorize (+0.38) but sacrificed Fuzzy/Noisy Recall.  Raw attention variants keep noise-robust advantages but never beat Gated-DeltaNet on capacity tasks.
2. This clear trade-off indicates that **magnitude information of the logits is useful for fuzzy/noisy matching**, while **probability normalisation is useful for stable memorisation capacity**.
3. None of the historical variants tried to **blend** the two regimes; they all picked one extreme (raw or fully-normalised).

Specific code issues identified
------------------------------
• In naive.py the intra-chunk causal attention is still an *unscaled dot-product*.  We either leave it raw (baseline) or replace it completely by softmax (Attempt-2).  There is no middle ground that can exploit both benefits.
• Consequently the model is forced into a rigid bias: noise-friendly but capacity-poor, or vice-versa.

Technical reasoning for the proposed change
------------------------------------------
New idea: **Blended Attention**
    attn = α · softmax(raw)  +  (1-α) · raw
with α = 0.7.
Rationale:
1. softmax(raw) provides bounded, stable weights ⇒ reduces variance of writes ⇒ better Compress & Memorize.
2. raw retains absolute scale clues that help Fuzzy/Noisy matching.
3. A convex mixture maintains differentiability and incurs negligible overhead (one extra softmax and a linear blend).
4. This is *different* from all previous attempts which used either raw, softmax, or gated writes, but never a simultaneous combination.
5. Causality is respected – we only change the read path; state update S still happens after output.

Expected improvements
---------------------
• Memorize should stay near the 0.78 level reached by pure-softmax because 70 % of the weight is still normalised.
• Fuzzy/Noisy Recall should recover much of the lost performance (raw component provides scale) – target ≥0.19 / 0.995.
• Compress should improve over baseline (>0.45) thanks to reduced variance.
• Selective Copy expected to remain ≈baseline (raw + normalised both work there).
• Overall average aims to surpass Gated-DeltaNet (≈0.70).

Lessons from Historical Attempts
--------------------------------
Patterns of successful approaches
• Capacity tasks benefit from mechanisms that stabilise attention magnitude (softmax, gating).
• Noise-robust tasks need raw magnitude information.
Unsuccessful patterns
• Pure global decay or hard gating harmed almost every metric.
• Full softmax or full raw attention forces an either/or trade-off.
How previous analyses informed current thinking
• Observing the Memorize gain from softmax and the robustness loss led to the insight that *partial* normalisation could reconcile both needs.
Consistently promising techniques
• Lightweight, parameter-free transformations placed on the read path; preserve original Delta rule write ordering.
Building upon / diverging from previous approaches
• Builds upon softmax success but diverges by mixing it with raw scores, an unexplored compromise that directly addresses the earlier trade-off.

name: delta_net_4layer_mix_raw_and_softmax_attention_in_output_in_for_loop
success: true