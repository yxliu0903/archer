                                                                Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                         delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1  delta_net_4layer_add_exponential_decay_to_S_before_update_in_chunk_loop  0.430250        0.998632      0.174114  0.400849      0.997338        0.950205
2        delta_net_4layer_add_softmax_to_intra_chunk_attention_in_for_loop  0.436064        0.985717      0.149983  0.779965      0.971766        0.951675
3                                                   gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908

20250613_074721

--- Insights ---
Key observations from experiment results
---------------------------------------
1. DeltaNet > Mamba/GLA on noisy-robust tasks (Fuzzy-Recall +14.9 pp, Noisy-Recall +9 pp) but still < Gated-DeltaNet on memory-capacity tasks (Compress −7.5 pp, Memorize −18 pp).
2. The raw intra-chunk attention used in the public DeltaNet implementation is an *unnormalised dot-product*.  As sequence length or key scale varies, the logits explode, causing a few very large q·k terms to dominate the weighted error signal u_i.  This produces high-variance memory updates and premature saturation – exactly what hurts Compress & Memorize.
3. Historical csv confirms this: vanilla DeltaNet (unnormalised) ≈0.44 / 0.40 on Compress / Memorize, while a naïve exponential-decay patch lowered both further, showing that *blunt* capacity reduction is not enough; we need *selective* normalisation.

Specific code issues identified
------------------------------
• In fla/ops/delta_rule/naive.py the line
    attn = (q_i @ k_i.T).masked_fill(mask, 0)
  feeds raw, potentially huge values into the update path.  No softmax or scaling is applied after masking, so magnitudes depend on d_k and data distribution.
• The global pre-computations (u, w) already include 1/√d_k scaling, but the per-chunk causal path does not, leading to mismatch between training gradients and inference range.

Technical reasoning for proposed changes
---------------------------------------
Modification performed: **apply softmax normalisation to the intra-chunk causal attention matrix** inside the for-loop **after masking**: 
    attn = q_i @ k_i.T
    attn.masked_fill_(attn_mask, –1e9)
    attn = softmax(attn, dim=-1)

Why this helps
1. Magnitude control – softmax converts logits to a probability simplex, preventing any single key from overwhelming the update term.
2. Interference reduction – the update becomes a convex combination of recent prediction errors (u_i); this attenuates spurious cross-talk and preserves capacity for new associations.
3. Stable gradients – bounded outputs (∈[0,1]) avoid exploding/vanishing gradients, improving convergence speed and final accuracy.
4. Causality preserved – read (o) computed before the memory write; we only insert a normalisation step, no extra look-ahead.

Expected improvements
• Compress: better long-range retention because the memory receives less noisy high-magnitude overwrites ⇒ expect >0.50 (beats Mamba 0.53 and approaches 0.57 of Gated).
• Memorize: fewer collisions will raise accuracy, target ≥0.55.
• Noise tasks: unchanged or slightly improved due to cleaner updates.
• Average: should surpass Gated-DeltaNet (>0.70) once tuned.

Lessons from Historical Attempts
-------------------------------
Successful patterns
• Techniques that *regularise* memory writes (gating/decay in Gated-DeltaNet) improve capacity-heavy tasks.
Unsuccessful patterns
• Global constant decay without adaptivity hurt both memory and noise robustness (Attempt 1).
How previous analyses informed current thinking
• We learnt that capacity problems stem from uncontrolled magnitude, not just accumulation.  Instead of another decay, we add *adaptive normalisation* (softmax) – a qualitatively different mechanism.
Consistently promising techniques
• Simple, parameter-free operations (1/√d_k, gating) that stabilise scale have shown large gains with minimal overhead.
Building upon/diverging
• We keep DeltaNet’s error-driven outer-product (strength on noise), diverge from fixed decay to probabilistic weighting, thus combining benefits of attention normalisation with Delta update.
