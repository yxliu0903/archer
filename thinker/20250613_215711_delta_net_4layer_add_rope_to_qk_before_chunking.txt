                                                                  Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                           delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1  delta_net_4layer_add_error_based_sigmoid_gate_to_state_update_in_for_loop  0.438554        0.996988      0.151126  0.378910      0.993787        0.856096
2    delta_net_4layer_add_exponential_decay_to_S_before_update_in_chunk_loop  0.430250        0.998632      0.174114  0.400849      0.997338        0.950205
3                      delta_net_4layer_add_layer_norm_to_qk_before_chunking  0.427726        0.998013      0.133315  0.270263      0.996733        0.979295
4                  delta_net_4layer_add_momentum_to_state_update_in_for_loop  0.432103        0.994521      0.164317  0.386064      0.992176        0.880826
5          delta_net_4layer_add_softmax_to_intra_chunk_attention_in_for_loop  0.436064        0.985717      0.160863  0.779965      0.971766        0.951675
6                                                     gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908

20250613_215711

--- Insights ---
Insights
========
Key observations from experiment results
---------------------------------------
1. Strengths & Weaknesses snapshot
   • DeltaNet: excellent on noise-robust tasks (Fuzzy-, Noisy-Recall, Selective-Copy) but under-performs on capacity / length-dependent tasks (Compress, Memorize).
   • Gated-DeltaNet: wins Compress & Memorize thanks to richer positional / decay handling; otherwise similar.
   • Prior modifications in historical attempts attacked scale control (softmax, layer-norm), forgetting (decay, momentum, error-gates) but **none added an explicit positional encoding**.  Missing positional signal likely caps Compress and long-distance Memorize performance.

2. csv history (accuracies_df.csv) confirms every successful model that helped Compress/Memorize (GLA, Gated-DeltaNet) incorporates some positional bias (global/local attention with decay) while plain DeltaNet variants that ignored position stayed ≤0.45.

Specific code issues identified
------------------------------
• naïve.py uses raw learned q,k projections only; relative position information must be learned from scratch, which is hard in long sequences and short training.
• Lack of explicit positional phase prevents the network from distinguishing identical content appearing at different time offsets, hampering compression and associative storage across long contexts.

Technical reasoning for the proposed changes
-------------------------------------------
Modification introduced: **apply Rotary Positional Embeddings (RoPE) to query & key tensors before chunking**.

Why RoPE?
1. Parameter-free, low-cost and proven to extrapolate sequence length.
2. Injects *relative* phase information directly into dot-products, improving ability to recombine memories based on distance ∆t.
3. Synergistic with Delta rule: write path (outer-product) unchanged, but keys carry richer geometry so stored memories encode position; read path immediately benefits from angle-aware similarity.
4. Distinct from previous attempts: tackles the missing capability (positional modelling) rather than scale or gating.

Expected improvements
---------------------
• Compress – better long-range reconstruction via phase-aware recall → predicted ≥0.55 (vs 0.44 baseline).
• Memorize – keys of different items less likely to collide → ≥0.60, aiming beyond Gated-DeltaNet 0.59.
• Noise tasks – RoPE is an orthogonal enhancement, should keep ≈baseline robustness.
• Average – target >0.72, surpassing Gated-DeltaNet.

Lessons from Historical Attempts
--------------------------------
Patterns of successful and unsuccessful approaches
• Successful: adding mechanisms that *structure* memory interactions (gating/decay, softmax) conditional on task demands.
• Unsuccessful: global constant decay or aggressive gating without adaptation impaired robustness.

How previous analyses informed current thinking
• Earlier trials showed that regulating scale alone cannot fully fix Compress.  Examination of Gated-DeltaNet’s advantage pointed to positional decay embedding.  Therefore, we pivot to injecting positional phase directly rather than more gates.

Consistently promising techniques / optimisations
• Lightweight, parameter-free augmentations that preserve causal order (RoPE, layer-norm) have low risk and good payoff.

Building upon or diverging from previous approaches
• Build upon realisation that memory collisions stem partly from indistinguishable positions; diverge by addressing *representation* rather than *magnitude* or *forgetting*.
