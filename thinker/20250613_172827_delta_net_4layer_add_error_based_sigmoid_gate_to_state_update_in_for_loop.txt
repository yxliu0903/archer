                                                                  Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                           delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1  delta_net_4layer_add_error_based_sigmoid_gate_to_state_update_in_for_loop  0.353399        0.996988      0.155280  0.378910      0.993787        0.278644
2    delta_net_4layer_add_exponential_decay_to_S_before_update_in_chunk_loop  0.430250        0.998632      0.174114  0.400849      0.997338        0.950205
3                  delta_net_4layer_add_momentum_to_state_update_in_for_loop  0.432103        0.994521      0.164317  0.386064      0.992176        0.880826
4          delta_net_4layer_add_softmax_to_intra_chunk_attention_in_for_loop  0.436064        0.985717      0.160863  0.779965      0.971766        0.951675
5                                                     gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908

20250613_172827

--- Insights ---
Insights
========
Key observations from experiment results
---------------------------------------
1. DeltaNet’s historical scores show a very polarized profile: near-perfect on Noisy-Recall (≈1.0) and Selective-Copy (≈0.95) but weak on Compress (0.44) and Memorize (0.40).  Previous fixes either –
   • Constant decay (Attempt-1) – damped everything ( − in all columns)
   • Softmax read normalisation (Attempt-2) – huge Memorize jump (+0.38) but harmed Fuzzy, Noisy, Selective tasks
   • Momentum on write (Attempt-3) – small/no gains, further losses
2. Comparing with Gated DeltaNet, the single most consistent advantage is an **adaptive forgetting gate** that preserves good memories while allowing necessary rewrites.  Our attempts so far have used *global* or *read-path* control; write-path remains unregulated in vanilla DeltaNet.

Specific code issues identified
------------------------------
• In fla/ops/delta_rule/naive.py  the memory write is  S ← S + kᵀu  every step, regardless of whether the current prediction error u is large or negligible.  Small but noisy errors accumulate and slowly overwrite useful long-term traces – the root cause of low Compress / Memorize.
• Earlier constant-rate decay reduced this interference but also deleted truly useful memories.
• There is no mechanism to modulate the **magnitude** of the update by its *information content* (error size).

Technical reasoning for proposed changes
---------------------------------------
NEW modification: **Error-Based Sigmoid Gate on the Write Path**
1. Compute a scalar gate g∈[0,1] from mean-squared error magnitude inside the current chunk:  g = σ(γ·‖u‖²).  When the model already predicts well (‖u‖ small) → g≈0 → keep S almost unchanged.  Large errors → g≈1 → allow full outer-product write.
2. Update rule becomes    S ← (1−g)·S + g·(kᵀu)
   • Behaves like an *adaptive replacement* reminiscent of the Delta Rule in neuroscience: only incorrect associations are overwritten.
3. Advantages
   • Selective forgetting: avoids catastrophic interference yet still frees capacity when needed.
   • Input-adaptive: unlike global γ decay, the gate is different each chunk and each (batch, head).
   • Very cheap: two reductions + sigmoid per chunk, no new learnable parameters except optional γ constant.
   • Causality preserved: output is produced before we modify S.
4. Distinct from previous attempts: regulates the *write* path (not read), and modulates *amplitude* (not just decay or normalise).

Expected improvements from this modification
-------------------------------------------
• Compress & Memorize – should rise because spurious low-error writes are suppressed, leaving room for genuinely new information.
• Fuzzy / Noisy Recall – unlike softmax attempt, we keep raw causal attention; noise robustness should stay high.
• Average – aim to surpass Gated DeltaNet (>0.70) by balancing capacity and robustness.

Lessons from Historical Attempts
--------------------------------
Patterns of successful approaches
• Mechanisms that regulate memory interaction (gates, normalisation) help capacity tasks.
• Over-aggressive global decay or un-tuned momentum hurts noise tasks.

How previous analyses informed current thinking
• Attempt-1 showed that *who* you forget matters – constant decay too blunt.
• Attempt-2 proved normalisation helps but touching read path can harm robustness.
• Attempt-3 indicated write-path manipulation matters but needs adaptive, not constant, scaling.

Consistently promising techniques / optimisations
• Lightweight, parameter-free gating operations; maintain causal ordering; tie control signal to meaningful statistics (error magnitude).

Building upon / diverging from previous approaches
• Build upon the insight that capacitance issues stem from uncontrolled writes.
• Diverge by replacing constant or momentum scaling with an *error-conditioned gate* – finer granularity, theoretically aligned with original Delta Rule ideology.
