                                                                Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                         delta_net_4layer  0.441687        0.998683      0.216574  0.403352      0.997438        0.949954
1  delta_net_4layer_add_exponential_decay_to_S_before_update_in_chunk_loop  0.430250        0.998632      0.174114  0.400849      0.997338        0.950205
2                delta_net_4layer_add_momentum_to_state_update_in_for_loop  0.432103        0.994521      0.164317  0.386064      0.992176        0.878838
3        delta_net_4layer_add_softmax_to_intra_chunk_attention_in_for_loop  0.436064        0.985717      0.160863  0.779965      0.971766        0.951675
4                                                   gated_delta_net_4layer  0.366612        0.999485      0.310357  0.586678      0.999478        0.996908

20250613_095038

--- Insights ---
Insights
========
Key observations from experiment results
---------------------------------------
1. DeltaNet baseline (accuracies_df.csv) shows two strong clusters:
   • High scores on error/noise–robust tasks (Noisy-Recall ≈1.00, Selective-Copy ≈0.95)
   • Weak on capacity-hungry tasks (Compress ≈0.44, Memorize ≈0.40).
2. Attempt-1 (constant decay) slightly *reduced* every metric – blunt forgetting is harmful.
3. Attempt-2 (softmax normalisation) fixed Memorize (+0.38!) but degraded all noise-robust tasks (-0.024 to ‑0.056) and still did not lift Compress.
   ⇒ We need a mechanism that
   • retains smooth gradients like softmax (to help Memorize) **but**
   • avoids destroying small-magnitude signals crucial for noisy/fuzzy tasks.

Specific code issues identified
------------------------------
• The outer-product update Δ = kᵀu can be *extremely spiky* when the prediction error u_i fluctuates, causing overwrites and hurting long-term storage (Compress).
• Previous fixes either damped everything (decay) or re-weighted reads (softmax) but none *smoothed the write signal itself*.
• The current file lacks any accumulator; each Δ is written in full to S, amplifying variance.

Technical reasoning for the proposed change
------------------------------------------
New idea: **Momentum-based state updates** (inspired by optimisation methods & TTT)  
Introduce a running exponential average M of recent updates:
    M ← μ·M + (1-μ)·Δ      (μ=0.9)
    S ← S + M
Why it should help:
1. Variance Reduction – momentum filters high-frequency noise in Δ, protecting existing memories (helps Compress & Fuzzy/Noisy Recall).
2. Gradient Amplification – sustained consistent errors accumulate in M, enabling large consolidated writes when truly needed (helps Memorize).
3. Parameter-free & cheap – just two extra tensor ops, constant memory.
4. Causality preserved – read (o) computed *before* S update; write still outer-product-based.
5. Distinct from earlier attempts – neither constant decay nor softmax on attention touched the *write pathway statistics*.

Expected improvements
---------------------
• Compress: less overwriting ⇒ >0.50.
• Memorize: keep ↑0.78 benefit from softmax but with fewer side-effects ⇒ stay ≥0.70.
• Fuzzy/Noisy Recall: smoother updates should restore previous ≈0.21/1.00 levels.
• Average expected ≥0.72, surpassing Gated DeltaNet (≈0.70).

Lessons from Historical Attempts
-------------------------------
Patterns of success / failure:
• Successful: mechanisms that *regularise memory interaction* (gating in Gated-DeltaNet, softmax normalisation) improve capacity tasks.
• Unsuccessful: indiscriminate decay or scaling that suppresses all signals.
How previous analyses informed current thinking:
• We saw that too-strong suppression (decay) hurts everywhere; selective control (softmax) helps but can distort noise-robust paths.  Momentum offers *selective smoothing*: high-frequency noise is damped, low-frequency signal preserved.
Consistently promising techniques:
• Lightweight, parameter-free operations (gates, averages) with correct causal ordering.
Building upon / diverging:
• Builds upon softmax finding that stabilising magnitude helps, but diverges by acting on *write* rather than *read* pathway, and keeping raw attention for noise tasks.
