Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                                             delta_net_4layer  0.487037        0.999515      0.189705  0.337827      0.999640        0.990733
1                               delta_net_4layer_adam_style_adaptive_state_update_after_output  0.086029        0.028959      0.011394  0.051220      0.009702        0.119096
2   delta_net_4layer_adaptive_exponential_decay_on_state_before_and_after_prediction_per_chunk  0.487829        0.999464      0.204738  0.341556      0.999550        0.990712
3                               delta_net_4layer_add_affine_free_layernorm_on_qk_before_l2norm  0.469601        0.999610      0.149512  0.341322      0.999526        0.992868
4                                   delta_net_4layer_add_momentum_to_state_update_after_output  0.483462        0.998907      0.126505  0.342439      0.997767        0.962976
5                                  delta_net_4layer_add_rope_to_keys_and_queries_before_l2norm  0.219386        0.655834      0.082418  0.272303      0.691661        0.516451
6                              delta_net_4layer_rmsnorm_on_error_per_chunk_before_state_update  0.459677        0.997880      0.130849  0.341811      0.996414        0.971792
7                                         delta_net_4layer_rope_softmax_intra_chunk_tanh_error  0.487571        0.986821      0.128179  0.832331      0.971426        0.993041
8                                     delta_net_4layer_row_norm_constrained_state_after_update  0.492459        0.999673      0.164167  0.342593      0.999622        0.985484
9                      delta_net_4layer_sigmoid_gate_on_state_update_per_chunk_using_mean_beta  0.484506        0.999680      0.125130  0.342419      0.999580        0.991628
10                                delta_net_4layer_softmax_intra_chunk_and_tanh_error_bounding  0.481904        0.985140      0.138232  0.841028      0.979918        0.984672
11                            delta_net_4layer_softmax_normalised_intra_chunk_causal_attention  0.477249        0.991150      0.138598  0.831789      0.980253        0.973402
12                                             delta_net_4layer_softmax_tanh_beta_gated_update  0.485169        0.996322      0.158361  0.812610      0.996470        0.996374
13                                          delta_net_4layer_tanh_on_error_before_state_update  0.494906        0.999610      0.158331  0.341603      0.999665        0.989535
14                                                                      gated_delta_net_4layer  0.408180        0.999918      0.303293  0.354294      0.999928        0.999304

20250617_034832

--- Insights ---
Insights

Key Observations from Experiment Results
• 历史 accuracies_df.csv 显示，以往的每个 Delta-Net 变体仅修复了一到两个薄弱任务，但从未能同时解决所有三个任务（Compress、Memorize、Fuzzy-Recall）。
    – Softmax-attention ⇒ Memorize 大幅提升（0.83），但 Fuzzy 依然较差。
    – tanh-clipped error ⇒ Fuzzy 表现更好（≈0.16），但 Memorize 无变化。
    – 对 state 进行 Row-norm ⇒ Compress/Memorize 有小幅提升，但 Fuzzy 无改善。
• 所有表现强劲的实验有一个共同点：对幅值（注意力权重、误差或状态）进行约束有帮助；尚未尝试将互补的约束结合起来。

Specific Code Issues Identified
1. 原始的块内点积未归一化，随着序列长度增加其和会爆炸——导致 Compress 和 Memorize 不稳定。
2. 预测误差 u 可能出现巨大尖峰——一个错误的 token 会覆盖整个记忆，导致 Fuzzy Recall 失效。
3. 状态矩阵 S 本身会持续增长；即使更新被约束，长期累积仍会破坏早期记忆。

Technical Reasoning for Proposed Changes
我们在单个文件中集成了三种轻量级、正交的正则化器：
1. Softmax 归一化的因果块内注意力（缩放 √d_k，掩蔽未来）——将局部混合限制在概率单纯形内 → 稳定 Memorize 和 Compress。
2. 在使用前，对每个（batch, head, chunk）的预测误差进行 RMS 归一化——保持方向，消除能量尖峰 → 保护 Fuzzy Recall，避免硬性饱和。
3. 每次更新后对记忆状态进行行最大范数约束——防止长期爆炸和干扰 → 支持 Compress 和 Memorize，并进一步保护 Fuzzy。
所有操作均无参数、可微分，并保持因果性：输出在 S 被修改前产生。

Expected Improvements
• Compress ≥ 0.52（行归一化 + softmax 降低干扰）。
• Memorize ≈ 0.8（继承 softmax 优势，同时保持状态稳定）。
• Fuzzy ≥ 0.26（RMS 归一化避免灾难性覆盖；行归一化保持相似性质量）。
• 鲁棒性任务（In-Context、Noisy、Selective Copy）应保持 ≳0.99，因为核心 delta 规则未变，更新只是更安全。
整体平均值应超过原始 DeltaNet（0.67）和 Gated-DeltaNet（0.60）。

Lessons from Historical Attempts
Successful Patterns
– 在任意阶段对幅值进行约束（softmax、tanh、row-norm）都能带来明显提升。
– 轻量级、可微分的调整，且遵循因果更新顺序，能保持 DeltaNet 的优势。
Unsuccessful Patterns
– 重型优化器式状态动态（Adam / momentum）会使选择性任务不稳定。
– 均匀衰减几乎无益。
Consistently Promising Techniques
– 结合正交的幅值控制方法，而不是堆叠相似方法。
– 利用已有统计量（局部 RMS、注意力 logits）进行自适应缩放。
Building on / Diverging from History
• 这是首次将三种互补的约束（注意力、误差、状态）结合在一起。每种单独使用都有效，合用应能同时解决所有薄弱任务。这与以往仅更改管道单一层的实验不同。

Name