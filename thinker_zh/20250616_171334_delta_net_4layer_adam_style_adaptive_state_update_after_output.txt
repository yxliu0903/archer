Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                                            delta_net_4layer  0.487037        0.999515      0.189705  0.337827      0.999640        0.990733
1  delta_net_4layer_adaptive_exponential_decay_on_state_before_and_after_prediction_per_chunk  0.487829        0.999464      0.204738  0.341556      0.999550        0.990712
2                                  delta_net_4layer_add_momentum_to_state_update_after_output  0.483462        0.998907      0.126505  0.342439      0.997767        0.962976
3                                                                      gated_delta_net_4layer  0.408180        0.999918      0.303293  0.354294      0.999928        0.999304

20250616_171334

--- Insights ---
Key Observations from Experiment Results  
实验结果的主要观察

• DeltaNet 在抗噪声任务（In-Context、Noisy、Selective-Copy）上表现突出，但在 Compress（0.49）、Memorize（0.34）以及尤其是 Fuzzy Recall（0.19）上仍然较弱。  
• 自适应衰减（Attempt-1）对 Compress/Memorize 的提升微乎其微，且未触及其根本原因：高方差的状态增长。  
• 动量缓冲（Attempt-2）将 Fuzzy Recall 提升至 0.23，但略微降低了 Compress，并严重损害了 Selective-Copy，表明简单的一阶平滑不足以解决问题——我们还需要**方差归一化**。

Specific Code Issues Identified  
发现的具体代码问题

1. Delta 更新 ΔS = kᵀu 在不同 token/head 之间可能相差数个数量级，因此即使有动量，单个极端误差仍可能主导 S。  
2. 缺乏针对每个记忆单元调整学习率的机制；β 已对更新进行了缩放，但跨时间的方差仍未受限。  
3. 当 S 爆炸时，Compress 和 Memorize 表现受损；当稀有更新被低估时，Fuzzy Recall 表现受损。

Technical Reasoning for Proposed Change: Adam-Style Adaptive State Update  
提出变更的技术理由：类 Adam 的自适应状态更新

• 将 ΔS_t 视为梯度，并为每个记忆单元维护一阶（m）和二阶（v）动量。  
   m_t = β1·m_{t-1}+ (1−β1)·ΔS_t  
   v_t = β2·v_{t-1}+ (1−β2)·ΔS_t²  
   S   = S + m̂_t / (√v̂_t+ϵ)  
• 按照最近的方差对每个单元进行归一化，防止出现巨大峰值，同时允许小而持续的更新累积。  
• 仍然**因果**：输出 o_i 在我们更新 S 之前就已完成。  
• 无需学习参数——β1、β2、ϵ 均为固定常数——因此训练过程保持不变。

Expected Improvements  
预期改进

• Compress & Memorize：状态幅值有界 → 干扰更少，召回更好（目标 ≥0.52 & ≥0.38）。  
• Fuzzy Recall：方差归一化放大了微弱但持续的噪声关联（目标 ≥0.28）。  
• 在抗噪声任务上保持 ≈1.0，因为自适应步长能保留其大且正确的更新。  
• 长度外推时的数值稳定性。

Lessons from Historical Attempts  
历史尝试的经验教训

成功模式  
– 基于误差的外积是核心。  – 能够调控幅值的门控/衰减或动量机制提升了保留能力。  
失败模式  
– 均匀衰减（Attempt-1）无法区分稳定信息与瞬时信息。  – 纯动量（Attempt-2）缺乏方差缩放，损害了选择性复制。  
持续有前景的技术  
– 能根据误差统计自适应调整的机制；保持因果性；轻量且可微分的修改。  
继承与创新  
– 我们保留动量（第一阶矩），但加入**二阶矩归一化**（Adam）——这是此前尝试中尚未探索的新方向——结合了 Attempt 1 和 2 的优点，同时避免了它们的缺陷。