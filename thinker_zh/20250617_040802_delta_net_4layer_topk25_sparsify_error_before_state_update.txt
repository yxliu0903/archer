Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                                             delta_net_4layer  0.487037        0.999515      0.189705  0.337827      0.999640        0.990733
1                               delta_net_4layer_adam_style_adaptive_state_update_after_output  0.086029        0.028959      0.011394  0.051220      0.009702        0.119096
2   delta_net_4layer_adaptive_exponential_decay_on_state_before_and_after_prediction_per_chunk  0.487829        0.999464      0.204738  0.341556      0.999550        0.990712
3                               delta_net_4layer_add_affine_free_layernorm_on_qk_before_l2norm  0.469601        0.999610      0.149512  0.341322      0.999526        0.992868
4                                   delta_net_4layer_add_momentum_to_state_update_after_output  0.483462        0.998907      0.126505  0.342439      0.997767        0.962976
5                                  delta_net_4layer_add_rope_to_keys_and_queries_before_l2norm  0.219386        0.655834      0.082418  0.272303      0.691661        0.516451
6                              delta_net_4layer_rmsnorm_on_error_per_chunk_before_state_update  0.459677        0.997880      0.130849  0.341811      0.996414        0.971792
7                                         delta_net_4layer_rope_softmax_intra_chunk_tanh_error  0.487571        0.986821      0.128179  0.832331      0.971426        0.993041
8                                     delta_net_4layer_row_norm_constrained_state_after_update  0.492459        0.999673      0.164167  0.342593      0.999622        0.985484
9                      delta_net_4layer_sigmoid_gate_on_state_update_per_chunk_using_mean_beta  0.484506        0.999680      0.125130  0.342419      0.999580        0.991628
10                                delta_net_4layer_softmax_intra_chunk_and_tanh_error_bounding  0.481904        0.985140      0.138232  0.841028      0.979918        0.984672
11                            delta_net_4layer_softmax_normalised_intra_chunk_causal_attention  0.477249        0.991150      0.138598  0.831789      0.980253        0.973402
12                          delta_net_4layer_softmax_rmsnorm_error_row_norm_state_after_update  0.470293        0.995246      0.135299  0.541736      0.981044        0.979639
13                                             delta_net_4layer_softmax_tanh_beta_gated_update  0.485169        0.996322      0.158361  0.812610      0.996470        0.996374
14                                          delta_net_4layer_tanh_on_error_before_state_update  0.494906        0.999610      0.158331  0.341603      0.999665        0.989535
15                                                                      gated_delta_net_4layer  0.408180        0.999918      0.303293  0.354294      0.999928        0.999304

20250617_040802

--- Insights ---
Key observations from experiment results  
• DeltaNet 的历史弱点在于灾难性干扰：一次高幅度的错误会写入一个稠密的外积，从而污染全局状态 S 的每一行。这对 Compress、Memorize 和 Fuzzy-Recall 的影响尤为严重。仅仅通过缩放（指数衰减、动量/Adam）或裁剪（tanh）整个误差的尝试，虽然提升了某一项任务，但却损害了另一项任务，因为这些方法在去除异常值的同时也抹杀了有用的小信号分量。  
• 检查 accuracies_df.csv 可以发现，每当我们约束更新落点的位置（例如 soft-max intra-chunk = 空间聚焦）时，Memorize 的表现都会显著提升（0.83），且不会丧失鲁棒性。模式非常清晰：关键在于修正的局部化——而不是均匀缩小。

Specific code issues identified (old naive.py)  
1. 预测误差 u_i 未经处理直接输入到输出和状态更新中；即使只有少数维度实际出错，它仍然是一个稠密的 D_v 向量。  
2. 因此，外积 kᵀ u_i 会修改 S 的所有行/列；微小的噪声分量会累积并模糊早期记忆。  
3. 没有机制来决定哪些维度值得修正；β 只缩放更新的“大小”，而不影响其稀疏性。

Technical reasoning for the proposed change  
我们在使用 u_i 之前引入了 Top-k 稀疏化：  
    u_i  ←  TopK(|u_i|, k) ⊙ u_i   （保留符号和值，其余置零）  
• k = ⌈0.25·d_v⌉（可配置）保留最显著的 25% 价值维度，这一比例在注意力剪枝文献中被认为是最佳点。  
• 优势：  
  – 更新集中在真正大的误差分量上，减少了无关维度的破坏性干扰。  
  – 微小但正确的维度保持不变，因此抗噪声任务不受影响。  
  – 与 tanh/clip 不同，保留的分量保持完整幅值 ⇒ 梯度信息充分。  
• 该更改无需额外参数，可微分，计算开销低（topk 长度≤64），并完全遵循因果性（在计算输出后、状态更新前进行稀疏化）。

Expected improvements  
• Compress & Memorize：写入的噪声维度更少 ⇒ 旧记忆得以保留；预期分别超过 0.52 和 0.38。  
• Fuzzy Recall：灾难性覆盖变得罕见 ⇒ 准确率有望提升至 0.26–0.28。  
• In-Context、Noisy Recall、Selective Copy：几乎无变化，因为这些任务的 u_i 已经稀疏（正确时接近零）。  
• 平均分数应超过基线 DeltaNet（0.668）和 Gated-DeltaNet（0.600）。

Lessons from Historical Attempts  
• 成功的方法总是对更新进行“局部化”或“约束”（soft-max、tanh、row-norm）。最佳 Memorize 结果来自高度聚焦的注意力。  
• 失败的方法（Adam/momentum）仅仅缩放了幅值，但更新仍然稠密，干扰依然存在。  
• 新的见解：干扰主要源于每个误差分量——即使极小——都会传播到 S 的每一行。对 u_i 进行稀疏化正是从根本上解决了这一问题，同时保留了幅值信息。  
• 我们在局部化（soft-max）成功的基础上，进一步将其应用到更早一层——即误差本身——这是过去 14 次尝试中未曾探索的方向。

Expected improvements & risk  
该修改应能与早期的幅值约束产生叠加效益，并且不太可能损害鲁棒性任务。风险在于 Top-k 对某些语义维度来说可能过小；我们默认设置为 25%，并保持可调。