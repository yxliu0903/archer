Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0        delta_net_4layer  0.487037        0.999515      0.189705  0.337827      0.999640        0.990733
1  gated_delta_net_4layer  0.408180        0.999918      0.303293  0.354294      0.999928        0.999304

20250616_161447

--- Insights ---
Key Observations from Experiment Results  
• DeltaNet 在上下文敏感和噪声回忆任务中表现出色（上下文回忆和噪声回忆任务均为 100%，选择性复制为 99%），但在压缩（Compress，48%）和记忆（Memorize，34%）任务上表现不佳。这两项任务要求长期保持信息且不受干扰——这是当前加性状态增长机制的弱点。  
• Gated-DeltaNet 通过引入衰减门控，在压缩（41%）和记忆（35%）任务上有所提升，表明某种形式的受控遗忘至关重要。然而，这会牺牲部分选择性复制的准确率。  
• 历史汇总数据（accuracies_df.csv）证实了这一模式：原始 DeltaNet 在噪声任务上准确率超过 90%，但在压缩任务上约为 0.48，而 Gated-DeltaNet 为 0.41——突出表现了保持与干扰之间的权衡。

Specific Code Issues Identified  
1. 状态饱和：S 只会增加（外积累加），没有机制来削弱陈旧或冲突的痕迹，导致长序列中出现干扰。  
2. 统一处理：所有位置的更新强度相同，无视误差大小；但 beta 已经指示了期望的更新强度，我们可以利用它来调节遗忘。  
3. 缺失时间先验：较早的记忆应逐渐消退，除非被强化；当前实现会无限期保留这些记忆。

Technical Reasoning for Proposed Change  
• 在每个块（chunk）上对 S 引入自适应指数衰减门控：S_decayed = γ · S，其中 γ = exp(–mean(beta_chunk))。  
  – 高 beta 块（预测误差大→覆盖多）会优先遗忘更多过去信息，减少串扰。  
  – 低 beta 块则保留记忆，避免不必要的稳定关联丢失。  
• 在计算预测/输出和写入新更新之前应用衰减。这符合因果性，并保持函数签名不变。  
• 衰减操作开销低（每个头每个块仅需一个标量），且可微分，因此训练过程中可以自动学习保持与遗忘的平衡。

Expected Improvements  
• 压缩（Compress）：噪声累积减少→重构准确率更高。  
• 记忆（Memorize）：键值对间干扰减少→乱序检索效果更好。  
• 对模糊/噪声回忆无负面影响，因为误差驱动部分未变；衰减甚至有助于过滤噪声。  
• 状态幅值趋于稳定，提升数值稳定性，并有助于推广到更长序列。

Lessons from Historical Attempts  
• 成功经验：门控/衰减机制（Gated-DeltaNet）提升了保持类任务表现→受控遗忘很重要。  
• 失败经验：纯加性更新（原始 DeltaNet）在长序列上导致记忆饱和。  
• 一贯有效的技术：误差驱动更新、分块处理、基于学习参数的门控。  
• 继承与创新：我们融合优势——保留 DeltaNet 的误差驱动外积机制，同时集成更轻量、由 beta 隐式学习的数据相关衰减，而非额外的 α_t 参数。这一做法区别于以往的硬编码门控，既保留了 DeltaNet 对噪声的高鲁棒性，又解决了干扰问题。