Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                                             delta_net_4layer  0.487037        0.999515      0.189705  0.337827      0.999640        0.990733
1                               delta_net_4layer_adam_style_adaptive_state_update_after_output  0.086029        0.028959      0.011394  0.051220      0.009702        0.119096
2   delta_net_4layer_adaptive_exponential_decay_on_state_before_and_after_prediction_per_chunk  0.487829        0.999464      0.204738  0.341556      0.999550        0.990712
3                               delta_net_4layer_add_affine_free_layernorm_on_qk_before_l2norm  0.469601        0.999610      0.149512  0.341322      0.999526        0.992868
4                                   delta_net_4layer_add_momentum_to_state_update_after_output  0.483462        0.998907      0.126505  0.342439      0.997767        0.962976
5                                  delta_net_4layer_add_rope_to_keys_and_queries_before_l2norm  0.219386        0.655834      0.082418  0.272303      0.691661        0.516451
6                              delta_net_4layer_rmsnorm_on_error_per_chunk_before_state_update  0.459677        0.997880      0.130849  0.341811      0.996414        0.971792
7                                         delta_net_4layer_rope_softmax_intra_chunk_tanh_error  0.487571        0.986821      0.128179  0.832331      0.971426        0.993041
8                                     delta_net_4layer_row_norm_constrained_state_after_update  0.492459        0.999673      0.164167  0.342593      0.999622        0.985484
9                      delta_net_4layer_sigmoid_gate_on_state_update_per_chunk_using_mean_beta  0.484506        0.999680      0.125130  0.342419      0.999580        0.991628
10                                delta_net_4layer_softmax_intra_chunk_and_tanh_error_bounding  0.481904        0.985140      0.138232  0.841028      0.979918        0.984672
11                            delta_net_4layer_softmax_normalised_intra_chunk_causal_attention  0.477249        0.991150      0.138598  0.831789      0.980253        0.973402
12                          delta_net_4layer_softmax_rmsnorm_error_row_norm_state_after_update  0.470293        0.995246      0.135299  0.541736      0.981044        0.979639
13                                             delta_net_4layer_softmax_tanh_beta_gated_update  0.485169        0.996322      0.158361  0.812610      0.996470        0.996374
14                                 delta_net_4layer_softmax_weighted_error_before_state_update  0.490721        0.999490      0.171971  0.340326      0.969874        0.968721
15                                          delta_net_4layer_tanh_on_error_before_state_update  0.494906        0.999610      0.158331  0.341603      0.999665        0.989535
16                                  delta_net_4layer_topk25_sparsify_error_before_state_update  0.445598        0.999629      0.188958  0.352690      0.999397        0.991736
17                                                                      gated_delta_net_4layer  0.408180        0.999918      0.303293  0.354294      0.999928        0.999304

20250617_052852

--- Insights ---
Insights

Key observations from experiment results  
• 最新的公开CSV再次确认了原始4层DeltaNet的长期弱点：Compress≈0.49，Memorize≈0.34，Fuzzy Recall≈0.19，而在In-Context、Noisy和Selective-Copy任务上已达到天花板水平（>0.99）。  
• 历史尝试（§History）显示出明显的模式：  
    – 限制更新幅度（tanh、RMS、行范数）有助于稳定性，但通常会去除有用信号，且很少能同时提升所有弱项任务。  
    – 局部化技巧（soft-max注意力、Top-k）极大提升了某一指标（如Memorize 0.83），但由于硬性阈值损害了其他指标。  
    – 记录的16种变体中，没有一种尝试过*统计归一化*，即仅去除全局均值/尺度，同时保留误差向量的完整方向信息。

Specific code issues identified  
1. 原始预测误差u = (attn @ v) − (w @ S) 可能具有较大的序列相关均值和方差。其密集的外积更新会将高方差能量引入记忆S的每一行，导致干扰，从而降低Compress和Memorize表现，并偶尔引发灾难性峰值，抹除Fuzzy-Recall。  
2. 先前的硬截断或稀疏化方法（tanh、Top-k）虽然解决了爆炸问题，但也破坏了梯度幅度或细微成分，导致各任务间权衡。  
3. 核心delta规则本身依然可靠；问题在于u的*统计漂移*。

Technical reasoning for proposed change  
我们在u上插入了无仿射的层归一化（每个token、每个head、每个chunk内部），*在其用于输出和状态更新之前*。  
    u_norm = (u − mean(u)) / sqrt(var(u)+eps)  
这保留了完整的方向信息（每个维度仍可贡献），但保证了零均值和单位方差，防止任何单个token向S注入过多能量。由于该变换在尺度上是平滑且可逆的，因此保留了梯度质量，避免了硬截断导致的细微信号丢失。  
预期效果：  
• Compress和Memorize——S中方差更小 ⇒ 早期记忆得以保留 ⇒ 分数应提升（>0.52 / >0.38）。  
• Fuzzy Recall——无灾难性覆盖 ⇒ 应提升至≥0.25，同时保持DeltaNet的噪声鲁棒性。  
• In-Context、Noisy、Selective-Copy——已表现优异；当误差较小时归一化近似恒等，因此性能应保持≃1.00。  
• 长度外推的数值稳定性应因状态增长受限而提升。

Expected improvements from the modification  
• 在不损害强项的前提下，三项历史弱任务均衡提升，使平均准确率超过基线DeltaNet（0.668）和Gated-DeltaNet（0.600）。

Lessons from Historical Attempts  
• 成功方法：轻量、可微分的幅度控制（soft-max、tanh、RMS、行范数），且遵循因果性。  
• 失败方法：重型优化器风格动态（Adam、动量）引入不稳定性；硬稀疏化丢失信号。  
• 一贯有效：约束统计量而非直接截断数值。  
• 基于历史：我们将幅度控制的经验与*统计*方法（LayerNorm）结合，既保留了完整信息，又是此前未曾采用的方式。这一做法不同于硬截断/稀疏化，延续了温和、无参数约束有益且无副作用的趋势。

name: delta_net_4layer_layernorm_on_error_before_state_update

success: true