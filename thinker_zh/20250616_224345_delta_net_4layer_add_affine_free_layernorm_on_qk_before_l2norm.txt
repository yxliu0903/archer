Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                                            delta_net_4layer  0.487037        0.999515      0.189705  0.337827      0.999640        0.990733
1                              delta_net_4layer_adam_style_adaptive_state_update_after_output  0.086029        0.028959      0.011394  0.051220      0.009702        0.119096
2  delta_net_4layer_adaptive_exponential_decay_on_state_before_and_after_prediction_per_chunk  0.487829        0.999464      0.204738  0.341556      0.999550        0.990712
3                                  delta_net_4layer_add_momentum_to_state_update_after_output  0.483462        0.998907      0.126505  0.342439      0.997767        0.962976
4                                 delta_net_4layer_add_rope_to_keys_and_queries_before_l2norm  0.219386        0.655834      0.082418  0.272303      0.691661        0.516451
5                            delta_net_4layer_softmax_normalised_intra_chunk_causal_attention  0.477249        0.991150      0.138598  0.831789      0.980253        0.973402
6                                                                      gated_delta_net_4layer  0.408180        0.999918      0.303293  0.354294      0.999928        0.999304

20250616_224345

--- Insights ---
Key Observations from Experiment Results  
• DeltaNet 已经在抗噪声任务（In-Context、Noisy、Selective-Copy ≈100%）上表现出色，但在 Compress（≈49%）、Memorize（≈34%）和 Fuzzy Recall（≈19%）方面表现受限。这三项任务在出现记忆干扰或特征尺度偏移导致 q·k 相似度不稳定时表现不佳，进而引发外积更新的异常波动。  
• 以往的尝试主要集中在状态级别的调整（衰减、动量、Adam、softmax）或位置编码（RoPE）。虽然这些方法对部分指标有轻微提升，但都未能带来全面改进；底层的表示统计不稳定问题依然未被解决。

Specific Code Issues Identified  
1. Query/Key 统计漂移：投影层的输出可能具有与序列相关的均值/方差。即使经过 L2 归一化，方向分布仍会发生变化，导致余弦相似度和更新幅度不一致。  
2. 缺乏特征级归一化：与使用 LayerNorm 的 Transformer 不同，DeltaNet 直接将原始投影输入到余弦归一化中。  
3. 由此导致的 (q·k) 及 (S k – v) 的高方差会削弱 Compress 和 Memorize 的学习效果，同时有时会对 Selective Copy 产生过强影响。

Technical Reasoning for Proposed Change  
• 在现有 L2 归一化之前，对 q 和 k 引入**无仿射 LayerNorm（eps=1e-5）**。这可以消除每个特征的均值/方差漂移，使方向向量在不同时间步和头之间更具可比性。  
• 稳定的余弦空间 ⇒ 更平滑的注意力 logits，更小的误差项 u 方差，以及更可预测的状态更新。  
• 不增加可学习参数，训练检查点保持兼容；计算开销极小（仅两个逐元素操作）。  
• 其他所有逻辑——包括因果块内混合、误差驱动的 delta 规则和状态更新顺序——均保持不变，以保留原有优势。

Expected Improvements  
• Compress & Memorize：减少过度放大特征的干扰，应能实现更干净、更长距离的存储（目标 >0.52 和 >0.38）。  
• Fuzzy Recall：在噪声下更一致的 key 匹配，预期可超过 ≥0.25。  
• 抗噪声任务保持或略有提升，因为全局机制未被改变。

Lessons from Historical Attempts  
• 成功模式：轻量级幅值控制（门控、softmax）和表示增强（RoPE）通常能在不损害鲁棒任务的前提下带来帮助。  
• 不成功的尝试：重型优化风格的状态更新（Adam）会破坏选择性任务的稳定性；统一衰减几乎无提升。  
• 一贯有效：归一化或有界操作能在保持因果性的同时平滑动态过程。  
• 基于此：我们不再沿用以往以状态为中心的调整，而是选择稳定**输入**——这是尚未被充分利用的方向。这一做法基于这样一个洞见：许多记忆错误源自表示漂移，而非更新公式本身。

Name