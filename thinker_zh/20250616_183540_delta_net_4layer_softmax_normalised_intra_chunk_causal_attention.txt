Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                                            delta_net_4layer  0.487037        0.999515      0.189705  0.337827      0.999640        0.990733
1                              delta_net_4layer_adam_style_adaptive_state_update_after_output  0.086029        0.028959      0.011394  0.051220      0.009702        0.119096
2  delta_net_4layer_adaptive_exponential_decay_on_state_before_and_after_prediction_per_chunk  0.487829        0.999464      0.204738  0.341556      0.999550        0.990712
3                                  delta_net_4layer_add_momentum_to_state_update_after_output  0.483462        0.998907      0.126505  0.342439      0.997767        0.962976
4                                 delta_net_4layer_add_rope_to_keys_and_queries_before_l2norm  0.219386        0.655834      0.082418  0.272303      0.691661        0.516451
5                                                                      gated_delta_net_4layer  0.408180        0.999918      0.303293  0.354294      0.999928        0.999304

20250616_183540

--- Insights ---
Insights

Key observations from experiment results  
• DeltaNet 在 In-Context（100）、Noisy（100）和 Selective-Copy（99）任务上表现强劲，但在 Compress（49）和 Memorize（34）上仍然较弱，在 Fuzzy Recall（19）上表现一般。  
• 历史上探索过的前三类修改方法：  
  – S 的自适应指数衰减（Attempt-1）→ 几乎没有变化。  
  – Momentum / Adam 风格的状态平滑（Attempts-2 & 3）→ 提升了 Fuzzy，但损害了其他任务，效果停滞。  
  – RoPE 位置信号注入（Attempt-4）仍在等待全面评估，但其针对的是不同的方向（表征而非状态）。  
• 一个常见的失败模式：序列长度增加时，块内相似度矩阵的幅值也随之增大；由于原始点积（余弦，因为 keys/queries 已 L2 归一化）被直接相加，权重分布可能变得分散或尖锐 → 更新幅度不稳定，导致干扰，从而损害 Compress/Memorize。

Specific code issue identified  
块内因果混合项为：  
    attn = (q_i @ k_iᵀ).masked_fill(mask, 0)  
这种原始相似度（范围约为 -1..1）被*线性*用于混合 u_i —— 实际上是一个有符号、无界的权重。当许多位置相似时，求和可能爆炸；当相似度较小时，混合过于分散。因此，模型缺乏对局部注意力的适当归一化。

Technical reasoning for the proposed change  
在块内注意力 logits 上添加温度缩放的 softmax（经典 Transformer 技巧）。步骤如下：  
1. 用 √d_k 缩放 logits 以保持尺度。  
2. 屏蔽未来位置以保持因果性。  
3. 应用 softmax，使每行概率 ∈[0,1] 且和为 1。

Why this should help  
• 有界权重 → 防止状态爆炸，避免 Compress/Memorize 任务失败。  
• 强调块内最相关的历史 token，减少干扰噪声。  
• 保持 S 的更新规则不变（输出后外积）。无额外可学习参数，极其轻量。  
• 与以往尝试正交（此前未对该项归一化）；因此，正如所要求的，提供了新的改进方向。

Expected improvements  
• Compress：更锐利的局部路由 + 稳定的幅值 → 期望重建分数更高（>0.52）。  
• Memorize：减少同一块内 key 之间的干扰 → >0.38。  
• Fuzzy Recall：key 噪声时更好区分 → >0.24。  
• 保持 In-Context/Noisy/Selective 任务约 1.0 的表现，因为全局误差驱动机制未变。

Lessons from Historical Attempts  
• 成功：控制幅值的机制（门控/衰减）或丰富表征（RoPE）。  
• 失败：重度优化技巧（Adam）导致 Selective Copy 不稳定；统一衰减几乎无增益。  
• 一贯有效：轻量、可微分的变体，能限制方差且不妨碍误差驱动学习。  
• 基于此：我们保留 DeltaNet 的优势，摒弃以往的状态平滑思路，引入历史上在多种架构中都表现稳健的经典注意力归一化。这是 MAD 运行中尚未尝试的新方向。