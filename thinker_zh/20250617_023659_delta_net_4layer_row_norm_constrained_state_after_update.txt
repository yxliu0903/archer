Unnamed: 0  Compress  Context Recall  Fuzzy Recall  Memorize  Noisy Recall  Selective Copy
0                                                                             delta_net_4layer  0.487037        0.999515      0.189705  0.337827      0.999640        0.990733
1                               delta_net_4layer_adam_style_adaptive_state_update_after_output  0.086029        0.028959      0.011394  0.051220      0.009702        0.119096
2   delta_net_4layer_adaptive_exponential_decay_on_state_before_and_after_prediction_per_chunk  0.487829        0.999464      0.204738  0.341556      0.999550        0.990712
3                               delta_net_4layer_add_affine_free_layernorm_on_qk_before_l2norm  0.469601        0.999610      0.149512  0.341322      0.999526        0.992868
4                                   delta_net_4layer_add_momentum_to_state_update_after_output  0.483462        0.998907      0.126505  0.342439      0.997767        0.962976
5                                  delta_net_4layer_add_rope_to_keys_and_queries_before_l2norm  0.219386        0.655834      0.082418  0.272303      0.691661        0.516451
6                                         delta_net_4layer_rope_softmax_intra_chunk_tanh_error  0.487571        0.986821      0.128179  0.832331      0.971426        0.993041
7                      delta_net_4layer_sigmoid_gate_on_state_update_per_chunk_using_mean_beta  0.484506        0.999680      0.125130  0.342419      0.999580        0.991628
8                                 delta_net_4layer_softmax_intra_chunk_and_tanh_error_bounding  0.481904        0.985140      0.138232  0.841028      0.979918        0.984672
9                             delta_net_4layer_softmax_normalised_intra_chunk_causal_attention  0.477249        0.991150      0.138598  0.831789      0.980253        0.973402
10                                             delta_net_4layer_softmax_tanh_beta_gated_update  0.485169        0.996322      0.158361  0.812610      0.996470        0.996374
11                                          delta_net_4layer_tanh_on_error_before_state_update  0.494906        0.999610      0.158331  0.341603      0.999665        0.989535
12                                                                      gated_delta_net_4layer  0.408180        0.999918      0.303293  0.354294      0.999928        0.999304

20250617_023659

--- Insights ---
Insights

Key Observations from Experiment Results
• 历史CSV文件确认了一个一致的模式：原始DeltaNet在噪声较大的任务中表现优异（在In-Context、Noisy、Selective-Copy任务上约为1.0），但其平均分被Compress（0.487）、Memorize（0.338）和Fuzzy Recall（0.190）拉低。
• 以往的尝试主要集中在：(i) 修改更新ΔS（衰减、动量、Adam、β门控），(ii) 用tanh/softmax约束误差u，或(iii) 丰富输入（RoPE、LayerNorm）。目前Compress的最佳成绩为0.495（tanh），Memorize为0.841（softmax-attention），Fuzzy为0.204（adaptive-decay）——没有任何变体能同时提升这三项任务。
• 一个常见的失败模式是状态幅值爆炸：一旦外积更新变大，旧信息就会被淹没，影响需要长期记忆或模糊匹配的任务表现。

Specific Code Issues Identified
1. 无法控制状态矩阵S每一*行*的L2范数增长。更新不断增加能量，缺乏任何约束机制。
2. 早期的修正仅约束了*更新*或*误差*，但累积的状态本身在长序列上仍可能爆炸——尤其是在Compress任务中，需要处理数百步后再回忆。
3. 由于q和k已经做了L2归一化，极大的S会导致点积过大，使预测误差u剧烈波动。

Technical Reasoning for the Proposed Change
• 在每次因果更新后，对S施加逐行单位范数约束。具体做法是，在`S = S + kᵀ·u`之后，计算每个key行的L2范数，如果‖row‖₂ > 1（或任意`max_row_norm`），则对该行进行重缩放。
• 这类似于权重归一化；既保持了表征能力，又防止了无控制的增长。
• 该方法无需额外参数，计算开销低（一次`norm`和一次除法），完全可微，并且保持因果性（先计算输出）。
• 预期效果：  
  – Compress & Memorize：早期记忆得以保留，因为后续更新无法无限制主导。  
  – Fuzzy Recall：较小、较早的关联不会被巨大行掩盖，余弦相似度依然有意义。  
  – 噪声鲁棒任务不受影响——其成功依赖于正确关联，而非大范数。

Expected Improvements
• Compress ≥ 0.52（超越以往所有结果）。• Memorize ≥ 0.38，且无softmax-only方案中的不稳定性。• Fuzzy Recall ≥ 0.24–0.26，缩小与Gated-DeltaNet的一半差距。• In-Context、Noisy、Selective-Copy任务保持≳0.99。整体平均分应超过0.70。

Lessons from Historical Attempts
• 成功经验：约束幅值（softmax、tanh）显著有效；无控制增长是主要敌人。
• 失败经验：优化器风格的状态动态（Adam/动量）增加了复杂性，但未能解决爆炸问题。
• 一贯有效：轻量级约束，不干扰delta规则逻辑，也不破坏因果性。
• 我们如何在以往工作基础上改进：不再约束*更新*（已尝试过），而是正则化*状态本身*。这一正交方向在已记录的11次尝试中尚未探索，且直接针对已识别的干扰根因。

Name

delta_net_4layer_row_norm_constrained_state_after_update

Success

true